{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA Gubernatorial and State Elections\n",
    "\n",
    "This is a new twitter scrape notebook for the California Gubernatorial and other state elections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gubernatorial\n",
    "The current list of announced and possible candidates that the project will be tracking are the ones listed on _La Times_ article [_California's next governor: Who's running, who's on the fence?_](http://www.latimes.com/politics/la-pol-ca-california-governor-list-2018-htmlstory.html) \n",
    "\n",
    "This is a list of the candidates listed in the article as of September 6th, 2017:\n",
    "\n",
    "**Declared**\n",
    "* [Gavin Newsom - D](https://twitter.com/GavinNewsom) [02/10/2015](http://www.latimes.com/local/politics/la-me-pol-gavin-newsom-20150212-story.html)\n",
    "* [John Chiang - D](https://twitter.com/JohnChiangCA) [05/17/2016](http://www.latimes.com/politics/la-pol-sac-essential-poli-john-chiang-jumps-into-californias-2018-governor-1463506797-htmlstory.html)\n",
    "* [Antonio Villaraigosa - D](https://twitter.com/antonio4ca) [11/10/2016](http://www.dailynews.com/2016/11/10/former-la-mayor-antonio-villaraigosa-launches-bid-for-california-governor/)\n",
    "* [Delaine Eastin - D](https://twitter.com/DelaineEastin) [11/01/2016](https://ballotpedia.org/Delaine_Eastin)\n",
    "* [John Cox - R](https://twitter.com/TheRealJohnHCox) [03/07/2017](https://en.wikipedia.org/wiki/John_H._Cox#2018_California_gubernatorial_election)\n",
    "* [Travis Allen- R](https://twitter.com/JoinTravisAllen) [06/22/2017](https://ballotpedia.org/Travis_Allen)\n",
    "* [Zoltan Istvan - L](https://twitter.com/zoltan_istvan) [02/12/2017](http://www.newsweek.com/zoltan-istvan-california-governor-libertarian-555088)\n",
    "\n",
    "**Potential**\n",
    "* [Kevin Faulconer - R](https://twitter.com/Kevin_Faulconer)\n",
    "* [Eric Garcetti - D](https://twitter.com/ericgarcetti)\n",
    "* [Tom Steyer - D](https://twitter.com/TomSteyer)\n",
    "* [Ashley Sweargin - R](https://twitter.com/ashleycvcf)\n",
    "* [Steve Westly - D](https://twitter.com/SteveWestly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senate\n",
    "This is a compiled list from our staff meetings and [Wikipedia](https://en.wikipedia.org/wiki/United_States_Senate_election_in_California,_2018) of likely candidates for the 2018 senate race along with people of interest in CA politics such as Kamala Harris and Jerry Brown.\n",
    "\n",
    "**Declared**\n",
    "* [Topher Brennan - D](https://twitter.com/tophertbrennan)\n",
    "* [Dianne Feinstein - D](https://twitter.com/SenFeinstein)\n",
    "* [Pat Harris - D](https://twitter.com/PatHarrisCA)\n",
    "* [David Hildebrand - D](https://twitter.com/David4SenateCA)\n",
    "* Douglas Howard Pierce - D (no twitter account)\n",
    "* [John Melendez - D](https://twitter.com/stutteringjohnm)\n",
    "* [Joe Sanberg - D](https://twitter.com/JosephNSanberg)\n",
    "* [Steve Stokes - D](https://twitter.com/Stokes4Senate)\n",
    "* [Kevin de León - D](https://twitter.com/kdeleon)\n",
    "* Timothy Charles Kalemkarian - R (no twitter account)\n",
    "* [Caren Lancona - R](https://twitter.com/Carenlancona4Se)\n",
    "* Stephen James Schrader - R (no twitter account)\n",
    "\n",
    "**Potential**\n",
    "* [Eric Garcetti - D](https://twitter.com/ericgarcetti)\n",
    "* [Loretta Sanchez - D](https://twitter.com/LorettaSanchez)\n",
    "* [Brad Sherman - D](https://twitter.com/BradSherman)\n",
    "* [Tom Steyer - D](https://twitter.com/TomSteyer)\n",
    "* [Eric Swalwell - D](https://twitter.com/RepSwalwell)\n",
    "* [Kevin Faulconer - R](https://twitter.com/Kevin_Faulconer)\n",
    "* [Caitlyn Jenner - R](https://twitter.com/Caitlyn_Jenner)\n",
    "* [Ashley Sweargin - R](https://twitter.com/ashleycvcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People of Interest\n",
    "This is a list of top California officials and politicians that may be useful for this research. \n",
    "\n",
    "**CA Exec**\n",
    "* [Jerry Brown - D](https://twitter.com/jerrybrowngov)\n",
    "* [Xavier Becerra - D](https://twitter.com/AGBecerra)\n",
    "* [Betty Yee - D](https://twitter.com/BettyYeeforCA)\n",
    "* [Dave Jones - D](https://twitter.com/CA_DaveJones)\n",
    "* [Tom Torlakson - D](https://twitter.com/TomTorlakson)\n",
    "\n",
    "**CA Legislature**\n",
    "* [Toni G. Atkins - D](https://twitter.com/toniatkins)\n",
    "* [Bill Monning - D](https://twitter.com/billmonning)\n",
    "* [Jean Fuller - R](https://twitter.com/JeanFuller)\n",
    "* [Anthony Rendon - D](https://twitter.com/Rendon63rd)\n",
    "* [Chris Holden - D](https://twitter.com/ChrisHoldenNews)\n",
    "* [Chad Mayes - R](https://twitter.com/ChadMayesCA)\n",
    "\n",
    "**CA Senate**\n",
    "* [Kamala Harris](https://twitter.com/KamalaHarris)\n",
    "\n",
    "**Party Leadership**\n",
    "* [Eric Bauman](https://twitter.com/EricBauman)\n",
    "* Jim Brulte - Inactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CA49 Congressional Race\n",
    "This is a contentious congressional race.\n",
    "\n",
    "**Democrat**\n",
    "* [Douglas Applegate](https://twitter.com/ApplegateCA49)\n",
    "* [Sara Jacobs](https://twitter.com/SaraJacobsCA)\n",
    "* [Paul Kerr](https://twitter.com/KerrForCongress)\n",
    "* [Mike Levin](https://twitter.com/MikeLevinCA)\n",
    "* [Christina Prejean](https://twitter.com/CPforCongress)\n",
    "\n",
    "**Republican**\n",
    "* [Rocky Chavez](https://twitter.com/AsmRocky)\n",
    "* [Kristin Gaspar](https://twitter.com/KristinDGaspar)\n",
    "* [Diane Harkey](https://twitter.com/DianeHarkey)\n",
    "* [Brian Maryott](https://twitter.com/brianlmaryott)\n",
    "* [Joshua Schoonover](https://twitter.com/JSSchoonover)\n",
    "\n",
    "**Libertarian**\n",
    "* Joshua Hancock\n",
    "\n",
    "**Peace and Freedom**\n",
    "* Jordan Mills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from unidecode import unidecode\n",
    "import xlsxwriter\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needs to be finished\n",
    "def illchar(par):\n",
    "  if (name == 'jimananich'):\n",
    "    writer = pd.ExcelWriter('id_.xlsx')\n",
    "    _id = pd.Series(df.iloc[:,0]).astype(str)\n",
    "    _id.to_excel(writer, sheet_name='id', header=False, index=False, float_format='string')\n",
    "    writer.save()\n",
    "    df.to_csv('illchar.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets ↓\n",
    "\n",
    "The functions below write data to the currently local sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Tweets and save them to respective excel file\n",
    "\n",
    "*twitter_list.xlsx* contains the list of candidates for each tweets excel file along with the metadata.\n",
    "\n",
    "*cand_tweets.xlsx* contains the tweets for all announced candidates\n",
    "\n",
    "*spec_tweets.xlsx* contains the tweets for all the speculated candidates\n",
    "\n",
    "*rep_tweets.xlsx* contains the tweets for all current CA represenatives of interest for the CA 2018 Elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# collect_data(tweet_sheet, twitter_list, twitter_sheet)\n",
    "# This function pulls new latest tweets and appends them to the correct excel file\n",
    "# params: tweet_sheet - the path to the excel file to save the new tweets \n",
    "#         twitter_list - path to file that contains the twitter handles, last tweet pulled id, tweet counts, and \n",
    "#                        last pull date\n",
    "#         twitter_sheet - the correct sheet of accounts corresponding to the tweet sheet passed in\n",
    "# returns: n/a\n",
    "def collect_data(tweet_sheet, twitter_list, twitter_sheet):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweets_path = os.path.expanduser(tweet_sheet)\n",
    "  twitter_path = os.path.expanduser(twitter_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(twitter_path)\n",
    "  list_df = pd.read_excel(twitter_path, twitter_sheet)\n",
    "  \n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweets_path)\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # check if rep has a twitter handle \n",
    "    if (type(name) is not unicode):\n",
    "      continue\n",
    "    \n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      \n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False, float_format='string')\n",
    "      \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(20)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  list_df.Last_ID = list_df.Last_ID.astype(str)\n",
    "  tweet_writer.save()\n",
    "  list_df.to_excel(list_writer, sheet_name=twitter_sheet, index=False, float_format='string')\n",
    "  list_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets for %s\", twitter_sheet)\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(tweet_sheet, tweet_list, sheetname):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweet_sheet = os.path.expanduser(tweet_sheet)\n",
    "  tweet_list = os.path.expanduser(tweet_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():  \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    if (updates_df.empty()):\n",
    "      continue \n",
    "      \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    tweets_df = update_metadata(tweets_df, updates_df, name)\n",
    "    \n",
    "    if (name == 'antonio4ca'):\n",
    "      continue\n",
    "      \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    # 100 second pause between data pulls to avoid token exceptions\n",
    "    time.sleep(20)\n",
    "  \n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "# currently not in use\n",
    "def update_metadata(tweets_df, updates_df, cand_name): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "  \n",
    "  ## loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['created_at'], inplace=True)\n",
    "  \n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_new_sheets(new_tweet_sheet, new_tweet_list, list_sheet_names):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  new_tweet_sheet = os.path.expanduser(new_tweet_sheet)\n",
    "  new_tweet_list = os.path.expanduser(new_tweet_list)\n",
    "  \n",
    "  # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "  tweet_writer = pd.ExcelWriter(new_tweet_sheet)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(new_tweet_list)\n",
    "  \n",
    "  for sheetname in list_sheet_names:\n",
    "    list_df = pd.read_excel(new_tweet_list, sheetname=sheetname)\n",
    "    list_df = list_df.dropna(axis=0, subset=['Twitter'])\n",
    "  \n",
    "    # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "    for row in list_df.itertuples():\n",
    "      name, since_id, count = row[2], row[3],row[4]\n",
    "      \n",
    "      # Create a Pandas dataframe from some data.\n",
    "      col_headers = {'id': [np.nan], 'created_at': [np.nan], 'text': [np.nan], 'hashtag#': [np.nan], 'at@': [np.nan],\n",
    "                    'link': [np.nan], 'retweets': [np.nan], 'favorites': [np.nan], 'fullURL': [np.nan]}\n",
    "      tweets_df = pd.DataFrame(col_headers)\n",
    "      tweets_df = tweets_df[['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'fullURL']]\n",
    "      \n",
    "      # write the updated data to the twitter profile's sheet to be saved\n",
    "      tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=0)\n",
    "    \n",
    "  tweet_writer.save()\n",
    "  logger.info(\"Done creating excel doc\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_xlsx_csv (tweet_sheet, sheetname, tweet_list):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4) \n",
    "  \n",
    "  #merged_corpus = pd.DataFrame(columns=['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'full URL'])\n",
    "  merged_df = pd.DataFrame()\n",
    "\n",
    "  initial_loop = True\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():\n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    if (initial_loop):\n",
    "      merged_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      merged_df['Name'] = name\n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      initial_loop = False \n",
    "    \n",
    "    else:\n",
    "      # read current cand tweet sheet\n",
    "      curr_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      curr_df['Name'] = name\n",
    "      #print (curr_df)\n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      \n",
    "      merged_df = merged_df.append(curr_df)\n",
    "      \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  merged_df.to_csv('merged_corpus.csv', encoding='utf-8')\n",
    "  return merged_df\n",
    "  \n",
    "  logger.info(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# func that will pull a number of tweets and create a coding sheet with columns for rating\n",
    "# sentitment, partisanship, policy, fact, and opinion\n",
    "def coding_sampling(dataframe, sample_size, exclude):\n",
    "  col_headers = {'sentiment': [np.nan], 'political': [np.nan], 'ideology': [np.nan],\n",
    "                 'macroeconomics': [np.nan], 'national_security': [np.nan], 'crime_law_enforcement': [np.nan],\n",
    "                 'civil_rights': [np.nan], 'environment': [np.nan], 'education': [np.nan], 'healthcare': [np.nan],\n",
    "                 'governance': [np.nan], 'no_policy_content': [np.nan], 'asks_for_donation': [np.nan],\n",
    "                 'ask_to_watch_read_share_follow_something': [np.nan], 'factual_claim': [np.nan], 'opinion': [np.nan]}\n",
    "  sample_df = pd.DataFrame(col_headers)\n",
    "  \n",
    "  # keep only id and text \n",
    "  dataframe = dataframe.iloc[:,0:1]\n",
    "  # make sure data columns fit format for final sampled set\n",
    "  dataframe = dataframe[['id', 'text']]\n",
    "  # if (exclude):\n",
    "    # remove already coded tweets\n",
    "  \n",
    "  sample_tweets = dataframe.sample(samplesize)\n",
    "  \n",
    "  sample_df.concat(sample_tweets, axis=1)\n",
    "  sample_df = sample_df[['id', 'text', 'sentiment', 'political', 'ideology', 'macroeconomics', 'national_security', \n",
    "                         'crime_law_enforcement', 'civil_rights', 'environment', 'education', 'healthcare', 'governance',\n",
    "                         'no_policy_content', 'asks_for_donation', 'ask_to_watch_read_share_follow_something', \n",
    "                         'factual_claim', 'opinion']]\n",
    "  return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates the large excel file to \n",
    "# create_new_sheets(state_data_dir + state_tweets, state_data_dir + twitter_list, state_sheets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Pull Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set file pathway variables an expand to HOME\n",
    "ca_data_dir = '~/Dropbox/Summer_of_Tweets/ca_working_sheets/'\n",
    "state_data_dir = '~/Dropbox/Summer_of_Tweets/State_Leg_Working_Data/'\n",
    "\n",
    "# the excel sheets containing the tweets\n",
    "cand_tweets = \"cand_tweets.xlsx\"\n",
    "spec_tweets = \"spec_tweets.xlsx\"\n",
    "rep_tweets = \"rep_tweets.xlsx\"\n",
    "state_tweets = \"state_tweets.xlsx\"\n",
    "\n",
    "# the excel file containing the accounts and its sheets\n",
    "twitter_list = \"Twitter_List.xlsx\" # this is the name of the metadata lists\n",
    "cand_sheet = \"cand\"\n",
    "spec_sheet = \"speculated\"\n",
    "rep_sheet = \"reps\"\n",
    "ca49_sheet = \"CA49\"\n",
    "state_sheets = [\"CA\", \"AK\", \"ME\", \"IL\", \"GA\", \"WY\", \"MI\", \"IN\",\"HI\", \"AL\", \"CT\", \"WA\", \"AZ\", \"FL\", \"NJ\", \"OR\", \"OK\",   \n",
    "                \"SD\", \"WI\", \"PA\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, cand_sheet)\n",
    "#print \n",
    "#collect_data(ca_data_dir + spec_tweets, ca_data_dir + twitter_list, spec_sheet)\n",
    "#print \n",
    "#collect_data(ca_data_dir + rep_tweets, ca_data_dir + twitter_list, rep_sheet)\n",
    "#print\n",
    "#collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collect_addition_data(ca_data_dir + cand_tweets, data_dir + twitter_list, cand_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + spec_tweets, data_dir + twitter_list, spec_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + rep_tweets, data_dir + twitter_list, rep_sheet) # <-- issues here\n",
    "print\n",
    "collect_addition_data(ca_data_dir + cand_tweets, data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for state in state_sheets:\n",
    "  collect_data(state_data_dir + state_tweets, state_data_dir + twitter_list, state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
