{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA Gubernatorial and State Elections\n",
    "\n",
    "This is a new twitter scrape notebook for the California Gubernatorial and other state elections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gubernatorial\n",
    "The current list of announced and possible candidates that the project will be tracking are the ones listed on _La Times_ article [_California's next governor: Who's running, who's on the fence?_](http://www.latimes.com/politics/la-pol-ca-california-governor-list-2018-htmlstory.html) \n",
    "\n",
    "This is a list of the candidates listed in the article as of September 6th, 2017:\n",
    "\n",
    "**Declared**\n",
    "* [Gavin Newsom - D](https://twitter.com/GavinNewsom) [02/10/2015](http://www.latimes.com/local/politics/la-me-pol-gavin-newsom-20150212-story.html)\n",
    "* [John Chiang - D](https://twitter.com/JohnChiangCA) [05/17/2016](http://www.latimes.com/politics/la-pol-sac-essential-poli-john-chiang-jumps-into-californias-2018-governor-1463506797-htmlstory.html)\n",
    "* [Antonio Villaraigosa - D](https://twitter.com/antonio4ca) [11/10/2016](http://www.dailynews.com/2016/11/10/former-la-mayor-antonio-villaraigosa-launches-bid-for-california-governor/)\n",
    "* [Delaine Eastin - D](https://twitter.com/DelaineEastin) [11/01/2016](https://ballotpedia.org/Delaine_Eastin)\n",
    "* [John Cox - R](https://twitter.com/TheRealJohnHCox) [03/07/2017](https://en.wikipedia.org/wiki/John_H._Cox#2018_California_gubernatorial_election)\n",
    "* [Travis Allen- R](https://twitter.com/JoinTravisAllen) [06/22/2017](https://ballotpedia.org/Travis_Allen)\n",
    "* [Zoltan Istvan - L](https://twitter.com/zoltan_istvan) [02/12/2017](http://www.newsweek.com/zoltan-istvan-california-governor-libertarian-555088)\n",
    "\n",
    "**Potential**\n",
    "* [Kevin Faulconer - R](https://twitter.com/Kevin_Faulconer)\n",
    "* [Eric Garcetti - D](https://twitter.com/ericgarcetti)\n",
    "* [Tom Steyer - D](https://twitter.com/TomSteyer)\n",
    "* [Ashley Sweargin - R](https://twitter.com/ashleycvcf)\n",
    "* [Steve Westly - D](https://twitter.com/SteveWestly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senate\n",
    "This is a compiled list from our staff meetings and [Wikipedia](https://en.wikipedia.org/wiki/United_States_Senate_election_in_California,_2018) of likely candidates for the 2018 senate race along with people of interest in CA politics such as Kamala Harris and Jerry Brown.\n",
    "\n",
    "**Declared**\n",
    "* [Topher Brennan - D](https://twitter.com/tophertbrennan)\n",
    "* [Dianne Feinstein - D](https://twitter.com/SenFeinstein)\n",
    "* [Pat Harris - D](https://twitter.com/PatHarrisCA)\n",
    "* [David Hildebrand - D](https://twitter.com/David4SenateCA)\n",
    "* Douglas Howard Pierce - D (no twitter account)\n",
    "* [John Melendez - D](https://twitter.com/stutteringjohnm)\n",
    "* [Joe Sanberg - D](https://twitter.com/JosephNSanberg)\n",
    "* [Steve Stokes - D](https://twitter.com/Stokes4Senate)\n",
    "* [Kevin de León - D](https://twitter.com/kdeleon)\n",
    "* Timothy Charles Kalemkarian - R (no twitter account)\n",
    "* [Caren Lancona - R](https://twitter.com/Carenlancona4Se)\n",
    "* Stephen James Schrader - R (no twitter account)\n",
    "\n",
    "**Potential**\n",
    "* [Eric Garcetti - D](https://twitter.com/ericgarcetti)\n",
    "* [Loretta Sanchez - D](https://twitter.com/LorettaSanchez)\n",
    "* [Brad Sherman - D](https://twitter.com/BradSherman)\n",
    "* [Tom Steyer - D](https://twitter.com/TomSteyer)\n",
    "* [Eric Swalwell - D](https://twitter.com/RepSwalwell)\n",
    "* [Kevin Faulconer - R](https://twitter.com/Kevin_Faulconer)\n",
    "* [Caitlyn Jenner - R](https://twitter.com/Caitlyn_Jenner)\n",
    "* [Ashley Sweargin - R](https://twitter.com/ashleycvcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People of Interest\n",
    "This is a list of top California officials and politicians that may be useful for this research. \n",
    "\n",
    "**CA Exec**\n",
    "* [Jerry Brown - D](https://twitter.com/jerrybrowngov)\n",
    "* [Xavier Becerra - D](https://twitter.com/AGBecerra)\n",
    "* [Betty Yee - D](https://twitter.com/BettyYeeforCA)\n",
    "* [Dave Jones - D](https://twitter.com/CA_DaveJones)\n",
    "* [Tom Torlakson - D](https://twitter.com/TomTorlakson)\n",
    "\n",
    "**CA Legislature**\n",
    "* [Toni G. Atkins - D](https://twitter.com/toniatkins)\n",
    "* [Bill Monning - D](https://twitter.com/billmonning)\n",
    "* [Jean Fuller - R](https://twitter.com/JeanFuller)\n",
    "* [Anthony Rendon - D](https://twitter.com/Rendon63rd)\n",
    "* [Chris Holden - D](https://twitter.com/ChrisHoldenNews)\n",
    "* [Chad Mayes - R](https://twitter.com/ChadMayesCA)\n",
    "\n",
    "**CA Senate**\n",
    "* [Kamala Harris](https://twitter.com/KamalaHarris)\n",
    "\n",
    "**Party Leadership**\n",
    "* [Eric Bauman](https://twitter.com/EricBauman)\n",
    "* Jim Brulte - Inactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CA49 Congressional Race\n",
    "This is a contentious congressional race.\n",
    "\n",
    "**Democrat**\n",
    "* [Douglas Applegate](https://twitter.com/ApplegateCA49)\n",
    "* [Sara Jacobs](https://twitter.com/SaraJacobsCA)\n",
    "* [Paul Kerr](https://twitter.com/KerrForCongress)\n",
    "* [Mike Levin](https://twitter.com/MikeLevinCA)\n",
    "* [Christina Prejean](https://twitter.com/CPforCongress)\n",
    "\n",
    "**Republican**\n",
    "* [Rocky Chavez](https://twitter.com/AsmRocky)\n",
    "* [Kristin Gaspar](https://twitter.com/KristinDGaspar)\n",
    "* [Diane Harkey](https://twitter.com/DianeHarkey)\n",
    "* [Brian Maryott](https://twitter.com/brianlmaryott)\n",
    "* [Joshua Schoonover](https://twitter.com/JSSchoonover)\n",
    "\n",
    "**Libertarian**\n",
    "* Joshua Hancock\n",
    "\n",
    "**Peace and Freedom**\n",
    "* Jordan Mills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from unidecode import unidecode\n",
    "import xlsxwriter\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets ↓\n",
    "\n",
    "The functions below write data to the currently local sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Tweets and save them to respective excel file\n",
    "\n",
    "*twitter_list.xlsx* contains the list of candidates for each tweets excel file along with the metadata.\n",
    "\n",
    "*cand_tweets.xlsx* contains the tweets for all announced candidates\n",
    "\n",
    "*spec_tweets.xlsx* contains the tweets for all the speculated candidates\n",
    "\n",
    "*rep_tweets.xlsx* contains the tweets for all current CA represenatives of interest for the CA 2018 Elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# collect_data(tweet_sheet, twitter_list, twitter_sheet)\n",
    "# This function pulls new latest tweets and appends them to the correct excel file\n",
    "# params: tweet_sheet - the path to the excel file to save the new tweets \n",
    "#         twitter_list - path to file that contains the twitter handles, last tweet pulled id, tweet counts, and \n",
    "#                        last pull date\n",
    "#         twitter_sheet - the correct sheet of accounts corresponding to the tweet sheet passed in\n",
    "# returns: n/a\n",
    "def collect_data(tweet_sheet, twitter_list, twitter_sheet):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweets_path = os.path.expanduser(tweet_sheet)\n",
    "  twitter_path = os.path.expanduser(twitter_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(twitter_path)\n",
    "  list_df = pd.read_excel(twitter_path, twitter_sheet)\n",
    "  \n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweets_path)\n",
    "  #tweet_writer.save()\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # check if rep has a twitter handle \n",
    "    if (type(name) is not unicode):\n",
    "      continue\n",
    "    \n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      \n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False)\n",
    "      tweet_writer.save()\n",
    "      \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "    \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(20)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  tweet_writer.save()\n",
    "  list_df.to_excel(list_writer, sheet_name=twitter_sheet, index=False)\n",
    "  list_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(tweet_sheet, tweet_list, sheetname):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweet_sheet = os.path.expanduser(tweet_sheet)\n",
    "  tweet_list = os.path.expanduser(tweet_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():  \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    if (updates_df.empty()):\n",
    "      continue \n",
    "      \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    tweets_df = update_metadata(tweets_df, updates_df, name)\n",
    "    \n",
    "    if (name == 'antonio4ca'):\n",
    "      continue\n",
    "      \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    # 100 second pause between data pulls to avoid token exceptions\n",
    "    time.sleep(20)\n",
    "  \n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "# currently not in use\n",
    "def update_metadata(tweets_df, updates_df, cand_name): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "  \n",
    "  ## loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['created_at'], inplace=True)\n",
    "  \n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_new_sheets(new_tweet_sheet, new_tweet_list, list_sheet_names):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  new_tweet_sheet = os.path.expanduser(new_tweet_sheet)\n",
    "  new_tweet_list = os.path.expanduser(new_tweet_list)\n",
    "  \n",
    "  # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "  tweet_writer = pd.ExcelWriter(new_tweet_sheet)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(new_tweet_list)\n",
    "  \n",
    "  for sheetname in list_sheet_names:\n",
    "    list_df = pd.read_excel(new_tweet_list, sheetname=sheetname)\n",
    "    list_df = list_df.dropna(axis=0, subset=['Twitter'])\n",
    "  \n",
    "    # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "    for row in list_df.itertuples():\n",
    "      name, since_id, count = row[2], row[3],row[4]\n",
    "      \n",
    "      # Create a Pandas dataframe from some data.\n",
    "      col_headers = {'id': [], 'created_at': [], 'text': [], 'hashtag#': [], 'at@': [],\n",
    "                    'link': [], 'retweets': [], 'favorites': [], 'fullURL': []}\n",
    "      tweets_df = pd.DataFrame(col_headers)\n",
    "      tweets_df = tweets_df[['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'fullURL']]\n",
    "      \n",
    "      # write the updated data to the twitter profile's sheet to be saved\n",
    "      tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=0)\n",
    "    \n",
    "  tweet_writer.save()\n",
    "  logger.info(\"Done creating excel doc\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/State_Leg_Working_Data/Twitter_List.xlsx\n",
      "INFO:__main__:Done creating excel doc\n",
      "INFO:__main__:Time Elapsed: 0\n"
     ]
    }
   ],
   "source": [
    "# this creates the large excel file to \n",
    "create_new_sheets(state_data_dir + state_tweets, state_data_dir + twitter_list, state_sheets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Pull Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set file pathway variables an expand to HOME\n",
    "ca_data_dir = '~/Dropbox/Summer_of_Tweets/ca_working_sheets/'\n",
    "state_data_dir = '~/Dropbox/Summer_of_Tweets/State_Leg_Working_Data/'\n",
    "\n",
    "# the excel sheets containing the tweets\n",
    "cand_tweets = \"cand_tweets.xlsx\"\n",
    "spec_tweets = \"spec_tweets.xlsx\"\n",
    "rep_tweets = \"rep_tweets.xlsx\"\n",
    "state_tweets = \"state_tweets.xlsx\"\n",
    "\n",
    "# the excel file containing the accounts and its sheets\n",
    "twitter_list = \"Twitter_List.xlsx\" # this is the name of the metadata lists\n",
    "cand_sheet = \"cand\"\n",
    "spec_sheet = \"speculated\"\n",
    "rep_sheet = \"reps\"\n",
    "ca49_sheet = \"CA49\"\n",
    "state_sheets = [\"AK\", \"ME\", \"IL\", \"GA\", \"WY\", \"MI\", \"IN\", \"HI\", \n",
    "                \"AL\", \"CT\", \"WA\", \"AZ\", \"FL\", \"NJ\", \"OR\", \"OK\",\n",
    "                \"SD\", \"WI\", \"PA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, cand_sheet)\n",
    "#print \n",
    "#collect_data(ca_data_dir + spec_tweets, ca_data_dir + twitter_list, spec_sheet)\n",
    "#print \n",
    "#collect_data(ca_data_dir + rep_tweets, ca_data_dir + twitter_list, rep_sheet)\n",
    "#print\n",
    "collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collect_addition_data(ca_data_dir + cand_tweets, data_dir + twitter_list, cand_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + spec_tweets, data_dir + twitter_list, spec_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + rep_tweets, data_dir + twitter_list, rep_sheet) <-- issues here\n",
    "print\n",
    "collect_addition_data(ca_data_dir + cand_tweets, data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/State_Leg_Working_Data/Twitter_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/State_Leg_Working_Data/state_tweets.xlsx\n",
      "INFO:__main__:Downloading 135 tweets from senatorcoghill\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0                   1  \\\n",
      "0    249621792900132864 2012-09-22 21:30:47   \n",
      "1    290361933767516160 2013-01-13 07:37:34   \n",
      "2    293093410678648832 2013-01-20 20:31:29   \n",
      "3    293438563440263168 2013-01-21 19:23:00   \n",
      "4    293489580190093313 2013-01-21 22:45:43   \n",
      "5    293810827432165376 2013-01-22 20:02:14   \n",
      "6    293833703849857024 2013-01-22 21:33:09   \n",
      "7    294122852377038849 2013-01-23 16:42:07   \n",
      "8    294218703887810561 2013-01-23 23:02:59   \n",
      "9    294477025920573440 2013-01-24 16:09:28   \n",
      "10   294857079833890816 2013-01-25 17:19:40   \n",
      "11   294994913106477057 2013-01-26 02:27:22   \n",
      "12   295668964036587520 2013-01-27 23:05:48   \n",
      "13   296334379595206657 2013-01-29 19:09:56   \n",
      "14   296422610823159808 2013-01-30 01:00:32   \n",
      "15   297041649362685953 2013-01-31 18:00:22   \n",
      "16   297108679029841921 2013-01-31 22:26:43   \n",
      "17   297409049580945408 2013-02-01 18:20:17   \n",
      "18   297468796589969409 2013-02-01 22:17:42   \n",
      "19   298514573345034240 2013-02-04 19:33:15   \n",
      "20   298825488632270848 2013-02-05 16:08:43   \n",
      "21   298861063934459905 2013-02-05 18:30:04   \n",
      "22   298976801747779585 2013-02-06 02:09:58   \n",
      "23   299193975280119808 2013-02-06 16:32:57   \n",
      "24   299209296514785280 2013-02-06 17:33:49   \n",
      "25   299590770690756608 2013-02-07 18:49:40   \n",
      "26   299646831296655360 2013-02-07 22:32:26   \n",
      "27   299666620933079040 2013-02-07 23:51:04   \n",
      "28   300376200533131265 2013-02-09 22:50:41   \n",
      "29   301063787111383041 2013-02-11 20:22:55   \n",
      "..                  ...                 ...   \n",
      "105  456923126274609153 2014-04-17 22:32:17   \n",
      "106  458791159553216513 2014-04-23 02:15:11   \n",
      "107  461969721433133056 2014-05-01 20:45:39   \n",
      "108  476060924017917952 2014-06-09 17:59:03   \n",
      "109  477876352100859904 2014-06-14 18:12:55   \n",
      "110  477900877043601408 2014-06-14 19:50:22   \n",
      "111  485260743244124160 2014-07-05 03:15:51   \n",
      "112  485264584157036544 2014-07-05 03:31:07   \n",
      "113  485273319654952963 2014-07-05 04:05:50   \n",
      "114  485273322750365696 2014-07-05 04:05:50   \n",
      "115  485273326160334848 2014-07-05 04:05:51   \n",
      "116  514994920231157761 2014-09-25 04:28:51   \n",
      "117  539253679455756288 2014-12-01 03:04:30   \n",
      "118  541122831602683904 2014-12-06 06:51:51   \n",
      "119  541127333545906177 2014-12-06 07:09:44   \n",
      "120  567409747175813120 2015-02-16 19:46:40   \n",
      "121  570631552545259520 2015-02-25 17:08:58   \n",
      "122  571081876061835264 2015-02-26 22:58:24   \n",
      "123  577904849762713600 2015-03-17 18:50:27   \n",
      "124  583781925464182784 2015-04-03 00:03:51   \n",
      "125  608038116650283008 2015-06-08 22:29:18   \n",
      "126  610504221666906112 2015-06-15 17:48:43   \n",
      "127  611253557673754624 2015-06-17 19:26:19   \n",
      "128  611265561662586881 2015-06-17 20:14:01   \n",
      "129  621826295383457792 2015-07-16 23:38:36   \n",
      "130  652241022659899392 2015-10-08 21:55:52   \n",
      "131  652241902410973188 2015-10-08 21:59:22   \n",
      "132  652242423385448448 2015-10-08 22:01:26   \n",
      "133  652245936207302656 2015-10-08 22:15:23   \n",
      "134  757989843330670592 2016-07-26 17:23:56   \n",
      "\n",
      "                                                     2 3  4  5   6  7  \n",
      "0    RT @FDNMdermot: Parnell wants to 'facilitate e...           1  0  \n",
      "1    The legislative session begins on Jan 15.  I'l...           0  0  \n",
      "2    Re: HB 69 - If they’re asking me to defend our...           0  0  \n",
      "3    In previous yrs, Fairbanks has been left out i...           1  0  \n",
      "4    I applaud FERC for continuing to cooperate wit...           1  0  \n",
      "5    RT @AKSenMajority: Sens. Coghill, Giessel, Bis...           1  0  \n",
      "6    Right to Life rally on the Capitol steps. http...           0  0  \n",
      "7    I will be closely following the F-16 issue and...           0  0  \n",
      "8    In our first Judiciary Committee meeting of th...           0  0  \n",
      "9    First In-State Energy Committee meeting starti...           0  0  \n",
      "10   RT @newsminer: Murkowski introduces bill to ke...           2  0  \n",
      "11   The Secretary of the Interior may very well be...           0  0  \n",
      "12   I know Fairbanks’ need, there’s no doubt about...           3  0  \n",
      "13   Today I'm meeting w/ students in the Junior Al...           2  0  \n",
      "14                                http://t.co/1nwdn3O4           0  0  \n",
      "15   LNG to cost $13.63 - $18.59 per Mcf... first t...           0  0  \n",
      "16   Susitna will be online by 2025 at soonest. Fbx...           1  0  \n",
      "17   I will be attending the Eielson EIS public mee...           1  0  \n",
      "18   Beginning Judiciary Committee in 10 minutes to...           1  0  \n",
      "19   Meeting with kids from the Wrangel School Dist...           0  0  \n",
      "20   Overview of the Alaska Stand Alone Pipeline (A...           0  0  \n",
      "21   According to AGDC, Fairbanks will receive gas ...           1  0  \n",
      "22   Yesterday I introduced SB 43 which raises the ...           1  0  \n",
      "23   Flying to Fairbanks.  Join me at the Princess ...           1  0  \n",
      "24   If confirmed, I urge Sec. of Interior Sally Je...           2  0  \n",
      "25   Just arrived back in Juneau.  Thanks to all wh...           1  0  \n",
      "26   Very thoughtful piece by @craigmedred in the D...           0  0  \n",
      "27   Joe Usibelli stopped by to talk Alaska's coal ...           2  1  \n",
      "28   Pleased to see Doyon drilling another explorat...           1  1  \n",
      "29   My bill (SB49) was just introduced in the Sena...           2  0  \n",
      "..                                                 ... .. .. .. .. ..  \n",
      "105  Gov. Parnell signing SB 49 with @RepLedoux1 an...           2  0  \n",
      "106  Today my crime/corrections/recidivism bill pas...           2  3  \n",
      "107  Former Representative Carl Moses passed away W...           2  1  \n",
      "108  I was pleased to deliver a commencement speech...           0  0  \n",
      "109  Join us for hotdogs at noon at the borough par...           0  0  \n",
      "110  Cutting grass at the borough park -- beautiful...           0  0  \n",
      "111  I posted 29 photos on Facebook in the album \"F...           1  0  \n",
      "112  Great weather for the North Pole parade. Hande...           2  0  \n",
      "113  I posted a new photo to Facebook http://t.co/R...           1  0  \n",
      "114  I posted a new photo to Facebook http://t.co/5...           1  0  \n",
      "115  I posted a new photo to Facebook http://t.co/U...           1  0  \n",
      "116  I posted 29 photos on Facebook in the album \"D...           0  0  \n",
      "117                             http://t.co/sYZcjOK365           0  0  \n",
      "118  I posted 24 photos on Facebook in the album \"L...           0  0  \n",
      "119  I posted 23 photos on Facebook in the album \"L...           0  0  \n",
      "120  It was nice to see some familiar faces from No...           1  1  \n",
      "121  I want to thank the Alaskans who came out in s...           2  0  \n",
      "122  Yesterday at the Senate Judiciary Crime Summit...           2  1  \n",
      "123  When it comes to the Endangered Species Act, \"...           1  0  \n",
      "124  My bill to expand protections for fire departm...           2  1  \n",
      "125  I'm trying to give some of the larger context ...           0  1  \n",
      "126  This summer, I’ll be asking my constituents a ...           0  0  \n",
      "127  Alaska Leaders Announce Initiative to Improve ...           1  0  \n",
      "128  Very pleased to be part of effort to reduce re...           2  1  \n",
      "129  Today's spending will be paid for by tomorrow'...           1  0  \n",
      "130  RT @AKSenMajority: Coghill: We need to ensure ...           1  0  \n",
      "131  RT @AKSenMajority: Coghill: Many of my constit...           5  0  \n",
      "132  RT @AKSenMajority: Kelly: The reform package t...           2  0  \n",
      "133  RT @AKSenMajority: Giessel: Expanding a broken...           2  0  \n",
      "134  Well-researched piece by @KTOOMatt on the new ...           1  3  \n",
      "\n",
      "[135 rows x 8 columns]\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "%d format: a number is required, not float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-94e147da026f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_sheets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_data_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstate_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_data_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtwitter_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-121-096ad444d133>\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(tweet_sheet, twitter_list, twitter_sheet)\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;32mprint\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet_writer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstartrow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0mtweet_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m       \u001b[0;31m# update since_id, count, and last_pull date in tweet list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/pandas/io/excel.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0mSave\u001b[0m \u001b[0mworkbook\u001b[0m \u001b[0mto\u001b[0m \u001b[0mdisk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \"\"\"\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     def write_cells(self, cells, sheet_name=None, startrow=0, startcol=0,\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/workbook/workbook.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0msave_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0msave_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/writer/excel.pyc\u001b[0m in \u001b[0;36msave_workbook\u001b[0;34m(workbook, filename)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZIP_DEFLATED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowZip64\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mwriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkbook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/writer/excel.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;34m\"\"\"Write data into the archive.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_archive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/writer/excel.pyc\u001b[0m in \u001b[0;36mwrite_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mARC_THEME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_theme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_worksheets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_chartsheets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/writer/excel.pyc\u001b[0m in \u001b[0;36m_write_worksheets\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mxml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0mrels_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_rels_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/worksheet/worksheet.pyc\u001b[0m in \u001b[0;36m_write\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcharts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrite_worksheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/writer/worksheet.pyc\u001b[0m in \u001b[0;36mwrite_worksheet\u001b[0;34m(worksheet)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mxf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSheetDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mxf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/.virtualenvs/fisp_twitter/lib/python2.7/site-packages/openpyxl/worksheet/worksheet.pyc\u001b[0m in \u001b[0;36mcalculate_dimension\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    446\u001b[0m         return '%s%d:%s%d' % (\n\u001b[1;32m    447\u001b[0m             \u001b[0mget_column_letter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_row\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m             \u001b[0mget_column_letter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_col\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m         )\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: %d format: a number is required, not float"
     ]
    }
   ],
   "source": [
    "for state in state_sheets:\n",
    "  collect_data(state_data_dir + state_tweets, state_data_dir + twitter_list, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
