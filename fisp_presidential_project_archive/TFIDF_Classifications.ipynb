{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sklearn.feature_extraction.text",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-aa03b39a578a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBernoulliNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sklearn.feature_extraction.text"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import re\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## functions for lexical analysis\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens,stemmer)\n",
    "    return stems\n",
    "\n",
    "def text2vec(full_text, use_stemmer=False):\n",
    "    \"\"\"\n",
    "    Convert text to vectors using TFIDF\n",
    "    ngram_range=(1,3) means unigrams, bigrams and trigrams; \n",
    "    if want to use bigrams only, define ngram_range=(2,2)\n",
    "    \n",
    "    \"\"\"\n",
    "    text=full_text[:]\n",
    "    if use_stemmer:\n",
    "        vectorizer = TfidfVectorizer(tokenizer=tokenize,ngram_range=(1,3))\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "    text_vector = vectorizer.fit_transform(text)\n",
    "    return text_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the Data\n",
    "Load \"human-coded-tweets\" into a pandas dataframe. Data preprocessing (convert to lower case, remove punctuation, remove screen names. etc) should be done before this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "data = pd.DataFrame.from_csv('ProcessedDataLowNoLinkNoPuncNoNames.csv')\n",
    "# take the 'text' column from the dataframe, and \n",
    "# convert the text to vectors\n",
    "full_text = data['text']\n",
    "text_vec = text2vec(full_text, use_stemmer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Split the data\n",
    "Split the data into a training set and a test set. Ideally the data should be shuffled before the split to avoid implicit bias in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_train = text_vec[0:6800,] # 6800 training tweets\n",
    "pred_test = text_vec[6800:,] # 725 test tweets\n",
    "pred_matrix = np.zeros((725,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Pick the right models\n",
    "The classification pipeline is: fitting the model with the training set -> predict labels on the test set -> compare predicted labels to real labels(human-coded-labels). These are all done on the human-coded-tweets, for the purpose of finding the best classification model with appropriate hyper-parameter settings. \n",
    "\n",
    "These could be done in one big for-loop, but it takes a long time to run, and it did crash my computer several times. What I did was to run classifications on only a few categories at a time - there would be many repeated code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentiment (Multinomial Naive Bayes); different alpha values would yield slightly different classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "==================================================\n",
      "Training Accuracy: 0.9910\n",
      "Test Accuracy: 0.6497\n",
      "Kappa: 0.3823\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.58      0.55      0.57       165\n",
      "          0       0.25      0.45      0.33        95\n",
      "          1       0.84      0.72      0.78       465\n",
      "\n",
      "avg / total       0.71      0.65      0.67       725\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "key = 'Sentiment'\n",
    "alpha = 0.4\n",
    "\n",
    "# real labels for the training set\n",
    "tar_train = data[key][0:6800,] \n",
    "# real labels for the test set\n",
    "tar_test = data[key][6800:,]\n",
    "\n",
    "# specify the classification model\n",
    "clf = MultinomialNB(alpha=alpha, fit_prior=True, class_prior=None)\n",
    "# fit the model with the training set\n",
    "clf.fit(pred_train, tar_train)\n",
    "# compute training accuracy\n",
    "train_score = clf.score(pred_train, tar_train)\n",
    "# predict labels on the test set\n",
    "y_pred = clf.predict(pred_test)\n",
    "\n",
    "# compute standard metrics\n",
    "test_accuracy = metrics.accuracy_score(tar_test, y_pred)\n",
    "class_report = metrics.classification_report(tar_test, y_pred)\n",
    "kappa = metrics.cohen_kappa_score(tar_test, y_pred)\n",
    "\n",
    "print(key)\n",
    "print('='*50)\n",
    "print('Training Accuracy: '+'{:.4f}'.format(train_score))\n",
    "print('Test Accuracy: '+'{:.4f}'.format(test_accuracy))\n",
    "print('Kappa: '+'{:.4f}'.format(kappa))\n",
    "print(class_report)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Political & Makes_a_Factual_or_Verifiable_Claim (Linear SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Political\n",
      "==================================================\n",
      "Training Accuracy: 0.9985\n",
      "Test Accuracy: 0.8662\n",
      "Kappa: 0.1062\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.00      0.00      0.00         1\n",
      "          0       0.47      0.08      0.14        95\n",
      "          1       0.88      0.99      0.93       629\n",
      "\n",
      "avg / total       0.82      0.87      0.82       725\n",
      "\n",
      "\n",
      "\n",
      "Makes_a_Factual_or_Verifiable_Claim\n",
      "==================================================\n",
      "Training Accuracy: 0.9960\n",
      "Test Accuracy: 0.7310\n",
      "Kappa: 0.3698\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.86      0.81       474\n",
      "          1       0.65      0.49      0.56       251\n",
      "\n",
      "avg / total       0.72      0.73      0.72       725\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kexin Chen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "keys = ['Political', 'Makes_a_Factual_or_Verifiable_Claim']\n",
    "\n",
    "for key in keys: \n",
    "    # real labels for the training set\n",
    "    tar_train = data[key][0:6800,] \n",
    "    # real labels for the test set\n",
    "    tar_test = data[key][6800:,]\n",
    "\n",
    "    # specify the classification model\n",
    "    clf = LinearSVC(class_weight='balanced')\n",
    "    # fit the model with the training set\n",
    "    clf.fit(pred_train, tar_train)\n",
    "    # compute training accuracy\n",
    "    train_score = clf.score(pred_train, tar_train)\n",
    "    # predict labels on the test set\n",
    "    y_pred = clf.predict(pred_test)\n",
    "\n",
    "    # compute standard metrics\n",
    "    test_accuracy = metrics.accuracy_score(tar_test, y_pred)\n",
    "    class_report = metrics.classification_report(tar_test, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(tar_test, y_pred)\n",
    "\n",
    "    print(key)\n",
    "    print('='*50)\n",
    "    print('Training Accuracy: '+'{:.4f}'.format(train_score))\n",
    "    print('Test Accuracy: '+'{:.4f}'.format(test_accuracy))\n",
    "    print('Kappa: '+'{:.4f}'.format(kappa))\n",
    "    print(class_report)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ideology & Immigration & Macroeconomic & National_Security & \n",
    "Crime & Civil_Rights & Environment & Education & Health_Care (Bagging Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ideology\n",
      "==================================================\n",
      "Training Accuracy: 0.9971\n",
      "Test Accuracy: 0.6510\n",
      "Kappa: 0.4446\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.73      0.59      0.65       144\n",
      "          0       0.52      0.63      0.57       238\n",
      "          1       0.74      0.69      0.71       343\n",
      "\n",
      "avg / total       0.66      0.65      0.65       725\n",
      "\n",
      "\n",
      "\n",
      "Immigration\n",
      "==================================================\n",
      "Training Accuracy: 0.9988\n",
      "Test Accuracy: 0.9807\n",
      "Kappa: 0.3578\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       707\n",
      "          1       1.00      0.22      0.36        18\n",
      "\n",
      "avg / total       0.98      0.98      0.97       725\n",
      "\n",
      "\n",
      "\n",
      "Macroeconomic\n",
      "==================================================\n",
      "Training Accuracy: 0.9993\n",
      "Test Accuracy: 0.9434\n",
      "Kappa: 0.5436\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      1.00      0.97       658\n",
      "          1       0.96      0.40      0.57        67\n",
      "\n",
      "avg / total       0.94      0.94      0.93       725\n",
      "\n",
      "\n",
      "\n",
      "National_Security\n",
      "==================================================\n",
      "Training Accuracy: 0.9991\n",
      "Test Accuracy: 0.9200\n",
      "Kappa: 0.4296\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.99      0.96       646\n",
      "          1       0.86      0.32      0.46        79\n",
      "\n",
      "avg / total       0.92      0.92      0.90       725\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kexin Chen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime\n",
      "==================================================\n",
      "Training Accuracy: 0.9997\n",
      "Test Accuracy: 0.9710\n",
      "Kappa: 0.0000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.99       704\n",
      "          1       0.00      0.00      0.00        21\n",
      "\n",
      "avg / total       0.94      0.97      0.96       725\n",
      "\n",
      "\n",
      "\n",
      "Civil_Rights\n",
      "==================================================\n",
      "Training Accuracy: 0.9996\n",
      "Test Accuracy: 0.9655\n",
      "Kappa: 0.1315\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98       699\n",
      "          1       0.67      0.08      0.14        26\n",
      "\n",
      "avg / total       0.96      0.97      0.95       725\n",
      "\n",
      "\n",
      "\n",
      "Environment\n",
      "==================================================\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy: 0.9917\n",
      "Kappa: 0.2482\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00       718\n",
      "          1       1.00      0.14      0.25         7\n",
      "\n",
      "avg / total       0.99      0.99      0.99       725\n",
      "\n",
      "\n",
      "\n",
      "Education\n",
      "==================================================\n",
      "Training Accuracy: 0.9997\n",
      "Test Accuracy: 0.9917\n",
      "Kappa: 0.2469\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00       719\n",
      "          1       0.50      0.17      0.25         6\n",
      "\n",
      "avg / total       0.99      0.99      0.99       725\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_samples=0.7\n",
    "max_features=0.8\n",
    "keys = ['Ideology', 'Immigration', 'Macroeconomic', 'National_Security', 'Crime', 'Civil_Rights',\n",
    "        'Environment', 'Education']\n",
    "\n",
    "for key in keys: \n",
    "    # real labels for the training set\n",
    "    tar_train = data[key][0:6800,] \n",
    "    # real labels for the test set\n",
    "    tar_test = data[key][6800:,]\n",
    "\n",
    "    # specify the classification model\n",
    "    clf = BaggingClassifier(LinearSVC(class_weight='balanced'), \n",
    "                            max_samples=max_samples, max_features=max_features)\n",
    "    # fit the model with the training set\n",
    "    clf.fit(pred_train, tar_train) \n",
    "    # compute training accuracy\n",
    "    train_score = clf.score(pred_train, tar_train)\n",
    "    # predict labels on the test set\n",
    "    y_pred = clf.predict(pred_test)\n",
    "\n",
    "    # compute standard metrics\n",
    "    test_accuracy = metrics.accuracy_score(tar_test, y_pred)\n",
    "    class_report = metrics.classification_report(tar_test, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(tar_test, y_pred)\n",
    "\n",
    "    print(key)\n",
    "    print('='*50)\n",
    "    print('Training Accuracy: '+'{:.4f}'.format(train_score))\n",
    "    print('Test Accuracy: '+'{:.4f}'.format(test_accuracy))\n",
    "    print('Kappa: '+'{:.4f}'.format(kappa))\n",
    "    print(class_report)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Governance & No_Policy_Content & Asks_for_Donation &  Asks_you_to_watch_something_share_something_follow_something & Misc & Expresses_an_Opinion (Bagging Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kexin Chen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Governance\n",
      "==================================================\n",
      "Training Accuracy: 0.9994\n",
      "Test Accuracy: 0.9697\n",
      "Kappa: 0.0000\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      1.00      0.98       703\n",
      "          1       0.00      0.00      0.00        22\n",
      "\n",
      "avg / total       0.94      0.97      0.95       725\n",
      "\n",
      "\n",
      "\n",
      "No_Policy_Content\n",
      "==================================================\n",
      "Training Accuracy: 0.9968\n",
      "Test Accuracy: 0.7034\n",
      "Kappa: 0.3987\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.90      0.44      0.59       353\n",
      "          1       0.64      0.95      0.77       372\n",
      "\n",
      "avg / total       0.77      0.70      0.68       725\n",
      "\n",
      "\n",
      "\n",
      "Asks_for_Donation\n",
      "==================================================\n",
      "Training Accuracy: 0.9997\n",
      "Test Accuracy: 0.9848\n",
      "Kappa: 0.2631\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99       712\n",
      "          1       1.00      0.15      0.27        13\n",
      "\n",
      "avg / total       0.99      0.98      0.98       725\n",
      "\n",
      "\n",
      "\n",
      "Asks_you_to_watch_something_share_something_follow_something\n",
      "==================================================\n",
      "Training Accuracy: 0.9987\n",
      "Test Accuracy: 0.9366\n",
      "Kappa: 0.6013\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.98      0.97       651\n",
      "          1       0.77      0.54      0.63        74\n",
      "\n",
      "avg / total       0.93      0.94      0.93       725\n",
      "\n",
      "\n",
      "\n",
      "Misc\n",
      "==================================================\n",
      "Training Accuracy: 0.9962\n",
      "Test Accuracy: 0.9531\n",
      "Kappa: 0.4275\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.98      0.98       690\n",
      "          1       0.52      0.40      0.45        35\n",
      "\n",
      "avg / total       0.95      0.95      0.95       725\n",
      "\n",
      "\n",
      "\n",
      "Expresses_an_Opinion\n",
      "==================================================\n",
      "Training Accuracy: 0.9954\n",
      "Test Accuracy: 0.7269\n",
      "Kappa: 0.3706\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.51      0.57       255\n",
      "          1       0.76      0.85      0.80       470\n",
      "\n",
      "avg / total       0.72      0.73      0.72       725\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_samples=0.7\n",
    "max_features=0.8\n",
    "keys = ['Governance', 'No_Policy_Content', 'Asks_for_Donation', \n",
    "        'Asks_you_to_watch_something_share_something_follow_something',\n",
    "        'Misc', 'Expresses_an_Opinion']\n",
    "\n",
    "for key in keys: \n",
    "    # real labels for the training set\n",
    "    tar_train = data[key][0:6800,] \n",
    "    # real labels for the test set\n",
    "    tar_test = data[key][6800:,]\n",
    "\n",
    "    # specify the classification model\n",
    "    clf = BaggingClassifier(LinearSVC(class_weight='balanced'),\n",
    "                            max_samples=max_samples, max_features=max_features)\n",
    "    # fit the model with the training set\n",
    "    clf.fit(pred_train, tar_train)\n",
    "    # compute training accuracy\n",
    "    train_score = clf.score(pred_train, tar_train)\n",
    "    # predict labels on the test set\n",
    "    y_pred = clf.predict(pred_test)\n",
    "\n",
    "    # compute standard metrics\n",
    "    test_accuracy = metrics.accuracy_score(tar_test, y_pred)\n",
    "    class_report = metrics.classification_report(tar_test, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(tar_test, y_pred)\n",
    "\n",
    "    print(key)\n",
    "    print('='*50)\n",
    "    print('Training Accuracy: '+'{:.4f}'.format(train_score))\n",
    "    print('Test Accuracy: '+'{:.4f}'.format(test_accuracy))\n",
    "    print('Kappa: '+'{:.4f}'.format(kappa))\n",
    "    print(class_report)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step Four: Final Classifications\n",
    "Having the appropriate models and hyper-parameter settings, we can use the models to predict labels for the entire corpus of tweets. At this step, we take all 7525 human-coded-tweets as the training set, and the whole corpus as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load training data and convert to a vector\n",
    "train_data = pd.DataFrame.from_csv('ProcessedDataLowNoLinkNoPuncNoNames.csv')\n",
    "train_text = train_data['text']\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize,ngram_range=(1,3))\n",
    "pred_train = vectorizer.fit_transform(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kexin Chen\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2821: DtypeWarning: Columns (8,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "# load the full corpus and convert to a vector\n",
    "final_data = pd.DataFrame.from_csv('ProcessedFullCorpus.csv')\n",
    "final_text = final_data['text']\n",
    "pred_final = vectorizer.transform(final_text.values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# specify classification models and hyper-parameter settings\n",
    "alpha = 0.1\n",
    "max_samples=0.7\n",
    "max_features=0.8\n",
    "\n",
    "def runClassifiers(classifier, pred_train, tar_train, pred_final):\n",
    "    if classifier=='NB':\n",
    "        clf = MultinomialNB(alpha=alpha, fit_prior=True, class_prior=None)\n",
    "    elif classifier=='SVC':\n",
    "        clf = LinearSVC(class_weight='balanced')\n",
    "    elif classifier=='Bag':\n",
    "        clf = BaggingClassifier(LinearSVC(class_weight='balanced'),\n",
    "                                max_samples=0.7, max_features=0.8)\n",
    "    clf.fit(pred_train,tar_train)\n",
    "    print('fit successfully')\n",
    "    y_class_final = clf.predict(pred_final)\n",
    "    print('predict successfully')\n",
    "    return y_class_final\n",
    "\n",
    "# A list hardcoded to specify the classifier to use for each category\n",
    "classifiers = ['NB','SVC','Bag','Bag','Bag','Bag','Bag','Bag','Bag',\n",
    "               'Bag','Bag','Bag','Bag','Bag','Bag','Bag','SVC','Bag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# obtain column names\n",
    "keys = [key for key in train_data if key != 'text']\n",
    "pred_matrix = np.zeros((len(final_data),18))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit successfully\n",
      "predict successfully\n",
      "Sentiment Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Political Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Ideology Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Immigration Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Macroeconomic Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "National_Security Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Crime Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Civil_Rights Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Environment Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Education Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Health_Care Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Governance Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "No_Policy_Content Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Asks_for_Donation Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Asks_you_to_watch_something_share_something_follow_something Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Misc Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Makes_a_Factual_or_Verifiable_Claim Classifications Done\n",
      "--------------------------------------------------\n",
      "fit successfully\n",
      "predict successfully\n",
      "Expresses_an_Opinion Classifications Done\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# final classifications: save prediction results in a matrix\n",
    "for n, key in enumerate(keys):\n",
    "    tar_train = train_data[key]\n",
    "    y_final = runClassifiers(classifiers[n], pred_train, tar_train, pred_final)\n",
    "    print(key+' Classifications Done')\n",
    "    print('-'*50)\n",
    "    pred_matrix[:,n] = y_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert prediction matrix into dataframe\n",
    "final_df = pd.DataFrame(pred_matrix).astype(int)\n",
    "final_df.columns = keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Political</th>\n",
       "      <th>Ideology</th>\n",
       "      <th>Immigration</th>\n",
       "      <th>Macroeconomic</th>\n",
       "      <th>National_Security</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Civil_Rights</th>\n",
       "      <th>Environment</th>\n",
       "      <th>Education</th>\n",
       "      <th>Health_Care</th>\n",
       "      <th>Governance</th>\n",
       "      <th>No_Policy_Content</th>\n",
       "      <th>Asks_for_Donation</th>\n",
       "      <th>Asks_you_to_watch_something_share_something_follow_something</th>\n",
       "      <th>Misc</th>\n",
       "      <th>Makes_a_Factual_or_Verifiable_Claim</th>\n",
       "      <th>Expresses_an_Opinion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7020</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141102</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126267</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108468</th>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Sentiment  Political  Ideology  Immigration  Macroeconomic  \\\n",
       "7020            1          1        -1            0              0   \n",
       "141102          1          1         1            0              0   \n",
       "1494            1          0         0            0              0   \n",
       "126267          1          1        -1            0              0   \n",
       "108468         -1          1         1            0              0   \n",
       "\n",
       "        National_Security  Crime  Civil_Rights  Environment  Education  \\\n",
       "7020                    0      0             0            0          0   \n",
       "141102                  0      0             0            0          0   \n",
       "1494                    0      0             0            0          0   \n",
       "126267                  0      0             0            0          0   \n",
       "108468                  0      0             0            0          0   \n",
       "\n",
       "        Health_Care  Governance  No_Policy_Content  Asks_for_Donation  \\\n",
       "7020              0           0                  1                  0   \n",
       "141102            0           0                  1                  0   \n",
       "1494              0           0                  1                  0   \n",
       "126267            0           0                  0                  0   \n",
       "108468            0           0                  0                  0   \n",
       "\n",
       "        Asks_you_to_watch_something_share_something_follow_something  Misc  \\\n",
       "7020                                                    0                0   \n",
       "141102                                                  0                0   \n",
       "1494                                                    0                0   \n",
       "126267                                                  0                0   \n",
       "108468                                                  1                0   \n",
       "\n",
       "        Makes_a_Factual_or_Verifiable_Claim  Expresses_an_Opinion  \n",
       "7020                                      0                     1  \n",
       "141102                                    1                     0  \n",
       "1494                                      0                     1  \n",
       "126267                                    1                     0  \n",
       "108468                                    0                     1  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save prediction result to csv\n",
    "out_file = 'finalPrediction.csv'\n",
    "final_df.to_csv(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
