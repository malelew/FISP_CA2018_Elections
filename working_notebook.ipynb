{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA Gubernatorial and State Elections\n",
    "\n",
    "This is a new twitter scrape notebook for the California Gubernatorial and other state elections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gubernatorial\n",
    "The current list of announced and possible candidates that the project will be tracking are the ones listed on _La Times_ article [_California's next governor: Who's running, who's on the fence?_](http://www.latimes.com/politics/la-pol-ca-california-governor-list-2018-htmlstory.html) \n",
    "\n",
    "This is a list of the candidates listed in the article as of September 6th, 2017:\n",
    "\n",
    "**Declared**\n",
    "* [Gavin Newsom - D](https://twitter.com/GavinNewsom) [02/10/2015](http://www.latimes.com/local/politics/la-me-pol-gavin-newsom-20150212-story.html)\n",
    "* [John Chiang - D](https://twitter.com/JohnChiangCA) [05/17/2016](http://www.latimes.com/politics/la-pol-sac-essential-poli-john-chiang-jumps-into-californias-2018-governor-1463506797-htmlstory.html)\n",
    "* [Antonio Villaraigosa - D](https://twitter.com/antonio4ca) [11/10/2016](http://www.dailynews.com/2016/11/10/former-la-mayor-antonio-villaraigosa-launches-bid-for-california-governor/)\n",
    "* [Delaine Eastin - D](https://twitter.com/DelaineEastin) [11/01/2016](https://ballotpedia.org/Delaine_Eastin)\n",
    "* [John Cox - R](https://twitter.com/TheRealJohnHCox) [03/07/2017](https://en.wikipedia.org/wiki/John_H._Cox#2018_California_gubernatorial_election)\n",
    "* [Travis Allen- R](https://twitter.com/JoinTravisAllen) [06/22/2017](https://ballotpedia.org/Travis_Allen)\n",
    "* [Zoltan Istvan - L](https://twitter.com/zoltan_istvan) [02/12/2017](http://www.newsweek.com/zoltan-istvan-california-governor-libertarian-555088)\n",
    "\n",
    "**Potential**\n",
    "* [Kevin Faulconer - R](https://twitter.com/Kevin_Faulconer)\n",
    "* [Eric Garcetti - D](https://twitter.com/ericgarcetti)\n",
    "* [Tom Steyer - D](https://twitter.com/TomSteyer)\n",
    "* [Ashley Sweargin - R](https://twitter.com/ashleycvcf)\n",
    "* [Steve Westly - D](https://twitter.com/SteveWestly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Senate\n",
    "This is a compiled list from our staff meetings and [Wikipedia](https://en.wikipedia.org/wiki/United_States_Senate_election_in_California,_2018) of likely candidates for the 2018 senate race along with people of interest in CA politics such as Kamala Harris and Jerry Brown.\n",
    "\n",
    "**Declared**\n",
    "* [Topher Brennan - D](https://twitter.com/tophertbrennan)\n",
    "* [Dianne Feinstein - D](https://twitter.com/SenFeinstein)\n",
    "* [Pat Harris - D](https://twitter.com/PatHarrisCA)\n",
    "* [David Hildebrand - D](https://twitter.com/David4SenateCA)\n",
    "* Douglas Howard Pierce - D (no twitter account)\n",
    "* [John Melendez - D](https://twitter.com/stutteringjohnm)\n",
    "* [Joe Sanberg - D](https://twitter.com/JosephNSanberg)\n",
    "* [Steve Stokes - D](https://twitter.com/Stokes4Senate)\n",
    "* [Kevin de León - D](https://twitter.com/kdeleon)\n",
    "* Timothy Charles Kalemkarian - R (no twitter account)\n",
    "* [Caren Lancona - R](https://twitter.com/Carenlancona4Se)\n",
    "* Stephen James Schrader - R (no twitter account)\n",
    "\n",
    "**Potential**\n",
    "* [Eric Garcetti - D](https://twitter.com/ericgarcetti)\n",
    "* [Loretta Sanchez - D](https://twitter.com/LorettaSanchez)\n",
    "* [Brad Sherman - D](https://twitter.com/BradSherman)\n",
    "* [Tom Steyer - D](https://twitter.com/TomSteyer)\n",
    "* [Eric Swalwell - D](https://twitter.com/RepSwalwell)\n",
    "* [Kevin Faulconer - R](https://twitter.com/Kevin_Faulconer)\n",
    "* [Caitlyn Jenner - R](https://twitter.com/Caitlyn_Jenner)\n",
    "* [Ashley Sweargin - R](https://twitter.com/ashleycvcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### People of Interest\n",
    "This is a list of top California officials and politicians that may be useful for this research. \n",
    "\n",
    "**CA Exec**\n",
    "* [Jerry Brown - D](https://twitter.com/jerrybrowngov)\n",
    "* [Xavier Becerra - D](https://twitter.com/AGBecerra)\n",
    "* [Betty Yee - D](https://twitter.com/BettyYeeforCA)\n",
    "* [Dave Jones - D](https://twitter.com/CA_DaveJones)\n",
    "* [Tom Torlakson - D](https://twitter.com/TomTorlakson)\n",
    "\n",
    "**CA Legislature**\n",
    "* [Toni G. Atkins - D](https://twitter.com/toniatkins)\n",
    "* [Bill Monning - D](https://twitter.com/billmonning)\n",
    "* [Jean Fuller - R](https://twitter.com/JeanFuller)\n",
    "* [Anthony Rendon - D](https://twitter.com/Rendon63rd)\n",
    "* [Chris Holden - D](https://twitter.com/ChrisHoldenNews)\n",
    "* [Chad Mayes - R](https://twitter.com/ChadMayesCA)\n",
    "\n",
    "**CA Senate**\n",
    "* [Kamala Harris](https://twitter.com/KamalaHarris)\n",
    "\n",
    "**Party Leadership**\n",
    "* [Eric Bauman](https://twitter.com/EricBauman)\n",
    "* Jim Brulte - Inactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CA49 Congressional Race\n",
    "This is a contentious congressional race.\n",
    "\n",
    "**Democrat**\n",
    "* [Douglas Applegate](https://twitter.com/ApplegateCA49)\n",
    "* [Sara Jacobs](https://twitter.com/SaraJacobsCA)\n",
    "* [Paul Kerr](https://twitter.com/KerrForCongress)\n",
    "* [Mike Levin](https://twitter.com/MikeLevinCA)\n",
    "* [Christina Prejean](https://twitter.com/CPforCongress)\n",
    "\n",
    "**Republican**\n",
    "* [Rocky Chavez](https://twitter.com/AsmRocky)\n",
    "* [Kristin Gaspar](https://twitter.com/KristinDGaspar)\n",
    "* [Diane Harkey](https://twitter.com/DianeHarkey)\n",
    "* [Brian Maryott](https://twitter.com/brianlmaryott)\n",
    "* [Joshua Schoonover](https://twitter.com/JSSchoonover)\n",
    "\n",
    "**Libertarian**\n",
    "* Joshua Hancock\n",
    "\n",
    "**Peace and Freedom**\n",
    "* Jordan Mills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from unidecode import unidecode\n",
    "import xlsxwriter\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# needs to be finished\n",
    "def illchar(par):\n",
    "  if (name == 'jimananich'):\n",
    "    writer = pd.ExcelWriter('id_.xlsx')\n",
    "    _id = pd.Series(df.iloc[:,0]).astype(str)\n",
    "    _id.to_excel(writer, sheet_name='id', header=False, index=False, float_format='string')\n",
    "    writer.save()\n",
    "    df.to_csv('illchar.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets ↓\n",
    "\n",
    "The functions below write data to the currently local sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Tweets and save them to respective excel file\n",
    "\n",
    "*twitter_list.xlsx* contains the list of candidates for each tweets excel file along with the metadata.\n",
    "\n",
    "*cand_tweets.xlsx* contains the tweets for all announced candidates\n",
    "\n",
    "*spec_tweets.xlsx* contains the tweets for all the speculated candidates\n",
    "\n",
    "*rep_tweets.xlsx* contains the tweets for all current CA represenatives of interest for the CA 2018 Elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "# collect_data(tweet_sheet, twitter_list, twitter_sheet)\n",
    "# This function pulls new latest tweets and appends them to the correct excel file\n",
    "# params: tweet_sheet - the path to the excel file to save the new tweets \n",
    "#         twitter_list - path to file that contains the twitter handles, last tweet pulled id, tweet counts, and \n",
    "#                        last pull date\n",
    "#         twitter_sheet - the correct sheet of accounts corresponding to the tweet sheet passed in\n",
    "# returns: n/a\n",
    "def collect_data(tweet_sheet, twitter_list, twitter_sheet):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweets_path = os.path.expanduser(tweet_sheet)\n",
    "  twitter_path = os.path.expanduser(twitter_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(twitter_path)\n",
    "  list_df = pd.read_excel(twitter_path, twitter_sheet)\n",
    "  \n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweets_path)\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # check if rep has a twitter handle \n",
    "    if (type(name) is not unicode):\n",
    "      continue\n",
    "    \n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      \n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False, float_format='string')\n",
    "      \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(20)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  list_df.Last_ID = list_df.Last_ID.astype(str)\n",
    "  tweet_writer.save()\n",
    "  list_df.to_excel(list_writer, sheet_name=twitter_sheet, index=False, float_format='string')\n",
    "  list_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets for %s\", twitter_sheet)\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(tweet_sheet, tweet_list, sheetname):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweet_sheet = os.path.expanduser(tweet_sheet)\n",
    "  tweet_list = os.path.expanduser(tweet_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():  \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    if (updates_df.empty()):\n",
    "      continue \n",
    "      \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    tweets_df = update_metadata(tweets_df, updates_df, name)\n",
    "    \n",
    "    if (name == 'antonio4ca'):\n",
    "      continue\n",
    "      \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    # 100 second pause between data pulls to avoid token exceptions\n",
    "    time.sleep(20)\n",
    "  \n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "# currently not in use\n",
    "def update_metadata(tweets_df, updates_df, cand_name): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "  \n",
    "  ## loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['created_at'], inplace=True)\n",
    "  \n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_new_sheets(new_tweet_sheet, new_tweet_list, list_sheet_names):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  new_tweet_sheet = os.path.expanduser(new_tweet_sheet)\n",
    "  new_tweet_list = os.path.expanduser(new_tweet_list)\n",
    "  \n",
    "  # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "  tweet_writer = pd.ExcelWriter(new_tweet_sheet)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(new_tweet_list)\n",
    "  \n",
    "  sheet_list = []\n",
    "  \n",
    "  for sheetname in list_sheet_names:\n",
    "    list_df = pd.read_excel(new_tweet_list, sheetname=sheetname)\n",
    "    list_df = list_df.dropna(axis=0, subset=['Twitter'])\n",
    "  \n",
    "    # \n",
    "    for row in list_df.itertuples():\n",
    "      name, since_id, count = row[2], row[3],row[4]\n",
    "      sheet_list.append(name)\n",
    "      \n",
    "      # Create a Pandas dataframe from some data.\n",
    "      col_headers = {'id': [np.nan], 'created_at': [np.nan], 'text': [np.nan], 'hashtag#': [np.nan], 'at@': [np.nan],\n",
    "                    'link': [np.nan], 'retweets': [np.nan], 'favorites': [np.nan], 'fullURL': [np.nan]}\n",
    "      tweets_df = pd.DataFrame(col_headers)\n",
    "      tweets_df = tweets_df[['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'fullURL']]\n",
    "      \n",
    "      # write the updated data to the twitter profile's sheet to be saved\n",
    "      tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=0)\n",
    "    \n",
    "  tweet_writer.save()\n",
    "  logger.info(\"Done creating excel doc\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)\n",
    "  return sheet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_xlsx_csv (tweet_sheet, sheetname, tweet_list):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweet_sheet = os.path.expanduser(tweet_sheet)\n",
    "  tweet_list = os.path.expanduser(tweet_list)  \n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  #merged_corpus = pd.DataFrame(columns=['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'full URL'])\n",
    "  merged_df = pd.DataFrame()\n",
    "\n",
    "  initial_loop = True\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():\n",
    "    name = row[0]\n",
    "    \n",
    "    if (initial_loop):\n",
    "      merged_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      merged_df['Name'] = name\n",
    "      num_tweets = len(merged_df.index)\n",
    "      print num_tweets\n",
    "\n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      initial_loop = False\n",
    "    \n",
    "    else:\n",
    "      # read current cand tweet sheet\n",
    "      curr_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      curr_df['Name'] = name\n",
    "      num_tweets = len(curr_df.index)\n",
    "      if (num_tweets == 0):\n",
    "        continue\n",
    "      \n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      \n",
    "      merged_df = merged_df.append(curr_df)\n",
    "  \n",
    "  print merged_df.shape\n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  #merged_df.to_csv('merged_corpus.csv', encoding='utf-8')\n",
    "  \n",
    "  logger.info(\"done\")\n",
    "  return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d9406338f6d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_to_number_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_df' is not defined"
     ]
    }
   ],
   "source": [
    "def list_to_number_string(value):\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        print str(value) + 'here'\n",
    "        return str(value)[1:-1]\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "sample_df['text'] = sample_df['text'].apply(list_to_number_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# func that will pull a number of tweets and create a coding sheet with columns for rating\n",
    "# sentitment, partisanship, policy, fact, and opinion\n",
    "def coding_sampling(coding_workbook, sheetname, sample_size, exclude):\n",
    "  logger.info(\"Start...\")\n",
    "  start = time.time()\n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  coding_workbook = os.path.expanduser(coding_workbook)\n",
    "  sample_writer = load_sheets(coding_workbook)\n",
    "  full_df = pd.read_excel(coding_workbook, sheetname)\n",
    "  logger.info(\"Full data loaded\")\n",
    "  \n",
    "  col_headers = {'id': [str],'text': [np.nan], 'sentiment': [np.nan], 'political': [np.nan], 'ideology': [np.nan],\n",
    "                 'macroeconomics': [np.nan], 'national_security': [np.nan], 'crime_law_enforcement': [np.nan],\n",
    "                 'civil_rights': [np.nan], 'environment': [np.nan], 'education': [np.nan], 'healthcare': [np.nan],\n",
    "                 'governance': [np.nan], 'no_policy_content': [np.nan], 'asks_for_donation': [np.nan],\n",
    "                 'ask_to_watch_read_share_follow_something': [np.nan], 'factual_claim': [np.nan], 'opinion': [np.nan]}\n",
    "  sample_df = pd.DataFrame(col_headers)\n",
    "  \n",
    "  # keep only id and text \n",
    "  full_df = full_df[['id', 'text']]\n",
    "  \n",
    "  # if (exclude):\n",
    "    # remove already coded tweets\n",
    "  \n",
    "  sample_tweets = full_df.sample(sample_size)\n",
    "  logger.info(\"Sample made\")\n",
    "  \n",
    "  sample_df = pd.concat([sample_tweets, sample_df])\n",
    "  sample_df = sample_df[['id', 'text', 'sentiment', 'political', 'ideology', 'macroeconomics', 'national_security', \n",
    "                         'crime_law_enforcement', 'civil_rights', 'environment', 'education', 'healthcare', 'governance',\n",
    "                         'no_policy_content', 'asks_for_donation', 'ask_to_watch_read_share_follow_something', \n",
    "                         'factual_claim', 'opinion']]\n",
    "  sample_df.drop(sample_df.tail(1).index,inplace=True) # drop last n rows\n",
    "  logger.info(\"Sample processed and concated\")\n",
    "  print sample_df.head()\n",
    "  \n",
    "  sample_df.id = sample_df.id.astype(str)\n",
    "  sample_df.to_excel(sample_writer, 'batch1', index=False, float_format='string')\n",
    "  sample_writer.save()\n",
    "  \n",
    "  logger.info(\"done\")\n",
    "  #return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/State_Leg_Working_Data/coding_file.xlsx\n",
      "INFO:__main__:Full data loaded\n",
      "INFO:__main__:Sample made\n",
      "INFO:__main__:Sample processed and concated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        id                                               text  \\\n",
      "335315  974655725854183424  RT @SenatorAOrr: Senate passed $6.6 billion Ed...   \n",
      "367935  492704480199073792  @AuburnTigers is partnering w/ End Child Hunge...   \n",
      "552395  542855653589794816  RT @n_cangelosi: Congrats to a former colleagu...   \n",
      "649013  679699932920569857  RT @b_roe: I try to preach like its someone's ...   \n",
      "525629  943238323363110913  “The legislation that I have filed for the las...   \n",
      "\n",
      "        sentiment  political  ideology  macroeconomics  national_security  \\\n",
      "335315        NaN        NaN       NaN             NaN                NaN   \n",
      "367935        NaN        NaN       NaN             NaN                NaN   \n",
      "552395        NaN        NaN       NaN             NaN                NaN   \n",
      "649013        NaN        NaN       NaN             NaN                NaN   \n",
      "525629        NaN        NaN       NaN             NaN                NaN   \n",
      "\n",
      "        crime_law_enforcement  civil_rights  environment  education  \\\n",
      "335315                    NaN           NaN          NaN        NaN   \n",
      "367935                    NaN           NaN          NaN        NaN   \n",
      "552395                    NaN           NaN          NaN        NaN   \n",
      "649013                    NaN           NaN          NaN        NaN   \n",
      "525629                    NaN           NaN          NaN        NaN   \n",
      "\n",
      "        healthcare  governance  no_policy_content  asks_for_donation  \\\n",
      "335315         NaN         NaN                NaN                NaN   \n",
      "367935         NaN         NaN                NaN                NaN   \n",
      "552395         NaN         NaN                NaN                NaN   \n",
      "649013         NaN         NaN                NaN                NaN   \n",
      "525629         NaN         NaN                NaN                NaN   \n",
      "\n",
      "        ask_to_watch_read_share_follow_something  factual_claim  opinion  \n",
      "335315                                       NaN            NaN      NaN  \n",
      "367935                                       NaN            NaN      NaN  \n",
      "552395                                       NaN            NaN      NaN  \n",
      "649013                                       NaN            NaN      NaN  \n",
      "525629                                       NaN            NaN      NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:done\n"
     ]
    }
   ],
   "source": [
    "coding_sampling(state_data_dir + coded_tweets, all_tweets, coding_sample_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reference code on how to save the data to an excel sheet and not lose id data\n",
    "writer = pd.ExcelWriter('coding_file.xlsx')\n",
    "merged_sheets.id = merged_sheets.id.astype(str)\n",
    "merged_sheets.to_excel(writer, 'total_sample', index=False, float_format='string')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Pull Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set file pathway variables an expand to HOME\n",
    "ca_data_dir = '~/Dropbox/Summer_of_Tweets/ca_working_sheets/'\n",
    "state_data_dir = '~/Dropbox/Summer_of_Tweets/State_Leg_Working_Data/'\n",
    "\n",
    "# the excel sheets containing the tweets\n",
    "cand_tweets = \"cand_tweets.xlsx\"\n",
    "spec_tweets = \"spec_tweets.xlsx\"\n",
    "rep_tweets = \"rep_tweets.xlsx\"\n",
    "state_tweets = \"state_tweets.xlsx\"\n",
    "coded_tweets = \"coding_file.xlsx\"\n",
    "\n",
    "# the excel file containing the accounts and its sheets\n",
    "twitter_list = \"Twitter_List.xlsx\" # this is the name of the metadata lists\n",
    "cand_sheet = \"cand\"\n",
    "spec_sheet = \"speculated\"\n",
    "rep_sheet = \"reps\"\n",
    "ca49_sheet = \"CA49\"\n",
    "state_sheets = [\"CA\", \"AK\", \"ME\", \"IL\", \"GA\", \"WY\", \"MI\", \"IN\",\"HI\", \"AL\", \"CT\", \"WA\", \"AZ\", \"FL\", \"NJ\", \"OR\", \"OK\",   \n",
    "                \"SD\", \"WI\", \"PA\"]\n",
    "all_handles = 'all'\n",
    "all_tweets = 'total_sample'\n",
    "\n",
    "coding_sample_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, cand_sheet)\n",
    "#print \n",
    "#collect_data(ca_data_dir + spec_tweets, ca_data_dir + twitter_list, spec_sheet)\n",
    "#print \n",
    "#collect_data(ca_data_dir + rep_tweets, ca_data_dir + twitter_list, rep_sheet)\n",
    "#print\n",
    "#collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "collect_addition_data(ca_data_dir + cand_tweets, data_dir + twitter_list, cand_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + spec_tweets, data_dir + twitter_list, spec_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + rep_tweets, data_dir + twitter_list, rep_sheet) # <-- issues here\n",
    "print\n",
    "collect_addition_data(ca_data_dir + cand_tweets, data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for state in state_sheets:\n",
    "  collect_data(state_data_dir + state_tweets, state_data_dir + twitter_list, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
