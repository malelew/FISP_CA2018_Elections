{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FISP Twitter Projects Notebook\n",
    "\n",
    "This notebook contains the latest code for pulling tweets and cleaning, manipilating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "# import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from unidecode import unidecode\n",
    "import xlsxwriter\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be finished\n",
    "def illchar(par):\n",
    "  writer = pd.ExcelWriter('id_.xlsx')\n",
    "  _id = pd.Series(df.iloc[:,0]).astype(str)\n",
    "  _id.to_excel(writer, sheet_name='id', header=False, index=False, float_format='string')\n",
    "  writer.save()\n",
    "  df.to_csv('illchar.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets â†“\n",
    "\n",
    "The functions below write data to the currently local sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Tweets and save them to respective excel file\n",
    "\n",
    "*twitter_list.xlsx* contains the list of candidates for each tweets excel file along with the metadata.\n",
    "\n",
    "*cand_tweets.xlsx* contains the tweets for all announced candidates\n",
    "\n",
    "*spec_tweets.xlsx* contains the tweets for all the speculated candidates\n",
    "\n",
    "*rep_tweets.xlsx* contains the tweets for all current CA represenatives of interest for the CA 2018 Elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# collect_data(tweet_sheet, twitter_list, twitter_sheet)\n",
    "# This function pulls new latest tweets and appends them to the correct excel file\n",
    "# params: tweet_sheet - the path to the excel file to save the new tweets \n",
    "#         twitter_list - path to file that contains the twitter handles, last tweet pulled id, tweet counts, and \n",
    "#                        last pull date\n",
    "#         twitter_sheet - the correct sheet of accounts corresponding to the tweet sheet passed in\n",
    "# returns: n/a\n",
    "def collect_data(tweet_sheet, twitter_list, twitter_sheet):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweets_path = os.path.expanduser(tweet_sheet)\n",
    "  twitter_path = os.path.expanduser(twitter_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(twitter_path)\n",
    "  list_df = pd.read_excel(twitter_path, twitter_sheet)\n",
    "  \n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') # used to fix datetime formatting\n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweets_path)\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # check if rep has a twitter handle \n",
    "    if (type(name) is not unicode):\n",
    "      continue\n",
    "    \n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    \n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      \n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False, float_format='string')\n",
    "      \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(20)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  list_df.Last_ID = list_df.Last_ID.astype(str)\n",
    "  list_df.to_excel(list_writer, sheet_name=twitter_sheet, index=False) #, float_format='string')\n",
    "  list_writer.save()\n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets for %s\", twitter_sheet)\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(tweet_sheet, tweet_list, sheetname):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweet_sheet = os.path.expanduser(tweet_sheet)\n",
    "  tweet_list = os.path.expanduser(tweet_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheet_name=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():  \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    if (updates_df.empty):\n",
    "      continue \n",
    "      \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    tweets_df = update_metadata(tweets_df, updates_df, name)\n",
    "    \n",
    "    if (name == 'antonio4ca'):\n",
    "      continue\n",
    "      \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    # 100 second pause between data pulls to avoid token exceptions\n",
    "    time.sleep(20)\n",
    "  \n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "# currently not in use\n",
    "def update_metadata(tweets_df, updates_df, cand_name): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "  \n",
    "  ## loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['created_at'], inplace=True)\n",
    "  \n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_sheets(new_tweet_sheet, new_tweet_list, list_sheet_names):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  new_tweet_sheet = os.path.expanduser(new_tweet_sheet)\n",
    "  new_tweet_list = os.path.expanduser(new_tweet_list)\n",
    "  \n",
    "  # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "  tweet_writer = pd.ExcelWriter(new_tweet_sheet)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(new_tweet_list)\n",
    "  \n",
    "  # will contain name of every accnt \n",
    "  sheet_list = []\n",
    "  \n",
    "  for sheetname in list_sheet_names:\n",
    "    list_df = pd.read_excel(new_tweet_list, sheetname=sheetname)\n",
    "    list_df = list_df.dropna(axis=0, subset=['Twitter'])\n",
    "  \n",
    "    # \n",
    "    for row in list_df.itertuples():\n",
    "      name, since_id, count = row[2], row[3],row[4]\n",
    "      sheet_list.append(name)\n",
    "      \n",
    "      # Create a Pandas dataframe from some data.\n",
    "      col_headers = {'id': [np.nan], 'created_at': [np.nan], 'text': [np.nan], 'hashtag#': [np.nan], 'at@': [np.nan],\n",
    "                    'link': [np.nan], 'retweets': [np.nan], 'favorites': [np.nan], 'fullURL': [np.nan]}\n",
    "      tweets_df = pd.DataFrame(col_headers)\n",
    "      tweets_df = tweets_df[['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'fullURL']]\n",
    "      \n",
    "      # write the updated data to the twitter profile's sheet to be saved\n",
    "      tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=0)\n",
    "    \n",
    "  tweet_writer.save()\n",
    "  logger.info(\"Done creating excel doc\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)\n",
    "  return sheet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# convert_xlsx_csv (tweet_sheet, sheetname, tweet_list)\n",
    "# This function takes the excel workbook of tweets and converts it all into a single csv of tweets\n",
    "# params: tweet_sheet - the path to the excel file with the tweets\n",
    "#         tweet_list - path to file that contains the twitter handles, last tweet pulled id, tweet counts, and \n",
    "#                        last pull date\n",
    "#         sheetname - sheet within in tweet_list\n",
    "# returns: n/a\n",
    "def convert_xlsx_csv (tweet_sheet, sheetname, tweet_list):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweet_sheet = os.path.expanduser(tweet_sheet)\n",
    "  tweet_list = os.path.expanduser(tweet_list)  \n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  #merged_corpus = pd.DataFrame(columns=['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'full URL'])\n",
    "  merged_df = pd.DataFrame()\n",
    "\n",
    "  initial_loop = True\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():\n",
    "    name = row[0]\n",
    "    \n",
    "    if (initial_loop):\n",
    "      merged_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      merged_df['Name'] = name\n",
    "      num_tweets = len(merged_df.index)\n",
    "      print num_tweets\n",
    "\n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      initial_loop = False\n",
    "    \n",
    "    else:\n",
    "      # read current cand tweet sheet\n",
    "      curr_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      curr_df['Name'] = name\n",
    "      num_tweets = len(curr_df.index)\n",
    "      if (num_tweets == 0):\n",
    "        continue\n",
    "      \n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      \n",
    "      merged_df = merged_df.append(curr_df)\n",
    "  \n",
    "  print merged_df.shape\n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  #merged_df.to_csv('merged_corpus.csv', encoding='utf-8')\n",
    "  \n",
    "  logger.info(\"done\")\n",
    "  return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I honestly don't remember what I used this for \n",
    "def list_to_number_string(value):\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        print str(value) + 'here'\n",
    "        return str(value)[1:-1]\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# sample_df['text'] = sample_df['text'].apply(list_to_number_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func that will pull a number of tweets and create a coding sheet with columns for rating\n",
    "# sentitment, partisanship, policy, fact, and opinion\n",
    "def coding_sampling(coding_workbook, sheetname, sample_size, exclude):\n",
    "  logger.info(\"Start...\")\n",
    "  start = time.time()\n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  coding_workbook = os.path.expanduser(coding_workbook)\n",
    "  sample_writer = load_sheets(coding_workbook)\n",
    "  full_df = pd.read_excel(coding_workbook, sheetname)\n",
    "  logger.info(\"Full data loaded\")\n",
    "  \n",
    "  col_headers = {'id': [str],'text': [np.nan], 'sentiment': [np.nan], 'political': [np.nan], 'ideology': [np.nan],\n",
    "                 'macroeconomics': [np.nan], 'national_security': [np.nan], 'ideology': [np.nan],\n",
    "                 'crime_law_enforcement': [np.nan], 'civil_rights': [np.nan], 'environment': [np.nan],\n",
    "                 'education': [np.nan], 'healthcare': [np.nan], 'governance': [np.nan], 'no_policy_content': [np.nan],\n",
    "                 'asks_for_donation': [np.nan], 'ask_to_watch_read_share_follow_something': [np.nan],\n",
    "                 'factual_claim': [np.nan], 'opinion': [np.nan]}\n",
    "  sample_df = pd.DataFrame(col_headers)\n",
    "  \n",
    "  # keep only id and text \n",
    "  full_df = full_df[['id', 'text']]\n",
    "  \n",
    "  # if (exclude):\n",
    "    # remove already coded tweets, yet to be implemented\n",
    "  \n",
    "  # sample the tweets\n",
    "  sample_tweets = full_df.sample(sample_size)\n",
    "  logger.info(\"Sample made\")\n",
    "  \n",
    "  # concat the sampled tweets with the coding measurments\n",
    "  sample_df = pd.concat([sample_tweets, sample_df])\n",
    "  sample_df = sample_df[['id', 'text', 'sentiment', 'political', 'ideology', 'macroeconomics', 'immigration',\n",
    "                         'national_security', 'crime_law_enforcement', 'civil_rights', 'environment', 'education',\n",
    "                         'healthcare', 'governance', 'no_policy_content', 'asks_for_donation',\n",
    "                         'ask_to_watch_read_share_follow_something', 'factual_claim', 'opinion']]\n",
    "  sample_df.drop(sample_df.tail(1).index,inplace=True) # drop last n rows\n",
    "  logger.info(\"Sample processed and concated\")\n",
    "  \n",
    "  sample_df.id = sample_df.id.astype(str)\n",
    "  sample_df.to_excel(sample_writer, 'batch2', index=False, float_format='string')\n",
    "  sample_writer.save()\n",
    "  \n",
    "  logger.info(\"done\")\n",
    "  #return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_sampling(state_data_dir + coded_tweets, all_tweets, coding_sample_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reference code on how to save the data to an excel sheet and not lose id data\n",
    "writer = pd.ExcelWriter('coding_file.xlsx')\n",
    "merged_sheets.id = merged_sheets.id.astype(str)\n",
    "merged_sheets.to_excel(writer, 'total_sample', index=False, float_format='string')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Pull Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file pathway variables an expand to HOME\n",
    "ca_data_dir = '~/Dropbox/Summer_of_Tweets/ca_working_sheets/'\n",
    "state_data_dir = '~/Dropbox/Summer_of_Tweets/State_Leg_Working_Data/'\n",
    "\n",
    "# the excel sheets containing the tweets\n",
    "cand_tweets = \"cand_tweets.xlsx\"\n",
    "spec_tweets = \"spec_tweets.xlsx\"\n",
    "rep_tweets = \"rep_tweets.xlsx\"\n",
    "state_tweets = \"state_tweets.xlsx\"\n",
    "coded_tweets = \"coding_file.xlsx\"\n",
    "\n",
    "# the excel file containing the accounts and its sheets\n",
    "twitter_list = \"Twitter_List.xlsx\" # this is the name of the metadata lists\n",
    "cand_sheet = \"cand\"\n",
    "spec_sheet = \"speculated\"\n",
    "rep_sheet = \"reps\"\n",
    "ca49_sheet = \"CA49\"\n",
    "state_sheets = [\"CA\", \"AK\", \"ME\", \"IL\", \"GA\", \"WY\", \"MI\", \"IN\",\"HI\", \"AL\", \"CT\", \"WA\", \"AZ\", \"FL\", \"NJ\", \"OR\", \"OK\",   \n",
    "                \"SD\", \"WI\", \"PA\"]\n",
    "all_handles = 'all'\n",
    "all_tweets = 'total_sample'\n",
    "\n",
    "coding_sample_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/ca_working_sheets/Twitter_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/ca_working_sheets/cand_tweets.xlsx\n",
      "INFO:__main__:Downloading 71 tweets from ApplegateCA49\n",
      "INFO:__main__:Updated new tweets on spreadsheet for ApplegateCA49\n",
      "INFO:__main__:Downloading 321 tweets from SaraJacobsCA\n",
      "INFO:__main__:Updated new tweets on spreadsheet for SaraJacobsCA\n",
      "INFO:__main__:Downloading 1405 tweets from MikeLevinCA\n",
      "INFO:__main__:Updated new tweets on spreadsheet for MikeLevinCA\n",
      "INFO:__main__:Downloading 3 tweets from CPforCongress\n",
      "INFO:__main__:Updated new tweets on spreadsheet for CPforCongress\n",
      "INFO:__main__:Downloading 124 tweets from KristinDGaspar\n",
      "INFO:__main__:Updated new tweets on spreadsheet for KristinDGaspar\n",
      "INFO:__main__:Downloading 58 tweets from DianeHarkey\n",
      "INFO:__main__:Updated new tweets on spreadsheet for DianeHarkey\n",
      "INFO:__main__:Downloading 65 tweets from brianlmaryott\n",
      "INFO:__main__:Updated new tweets on spreadsheet for brianlmaryott\n",
      "INFO:__main__:Done appending new tweets for CA49\n",
      "INFO:__main__:Time Elapsed: 3\n"
     ]
    }
   ],
   "source": [
    "collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, cand_sheet)\n",
    "print \n",
    "collect_data(ca_data_dir + spec_tweets, ca_data_dir + twitter_list, spec_sheet)\n",
    "print \n",
    "collect_data(ca_data_dir + rep_tweets, ca_data_dir + twitter_list, rep_sheet)\n",
    "print\n",
    "collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect_addition_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, cand_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + spec_tweets, ca_data_dir + twitter_list, spec_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + rep_tweets, ca_data_dir + twitter_list, rep_sheet) # <-- issues here\n",
    "print\n",
    "collect_addition_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for state in state_sheets:\n",
    "  collect_data(state_data_dir + state_tweets, state_data_dir + twitter_list, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
