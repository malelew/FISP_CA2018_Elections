{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FISP Twitter Projects Notebook\n",
    "\n",
    "This notebook contains the latest code for pulling tweets and cleaning, manipilating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary python packages\n",
    "import sys\n",
    "#sys.path.append(\"/usr/local/lib/python2.7/site-packages\")\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "# import dropbox #https://www.dropbox.com/developers-v1/core/docs/python\n",
    "import csv\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import logging\n",
    "import gspread\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from unidecode import unidecode\n",
    "import xlsxwriter\n",
    "\n",
    "#Twitter and Dropbox API credentials\n",
    "import api_cred as ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup debug logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify print precison for easier debugging\n",
    "np.set_printoptions(precision=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_twitter():\n",
    "  auth = tweepy.OAuthHandler(ac.consumer_key, ac.consumer_secret)\n",
    "  auth.set_access_token(ac.access_key, ac.access_secret)\n",
    "  api = tweepy.API(auth)\n",
    "  return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_tweets(tweet_name, since_id):\n",
    "  api = authenticate_twitter()\n",
    "  tweets = []\n",
    "  new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200)\n",
    "  tweets.extend(new_tweets)\n",
    "  if len(tweets) > 0:\n",
    "    max_id = tweets[-1].id - 1\n",
    "  while (len(new_tweets) > 0):\n",
    "    new_tweets = api.user_timeline(screen_name = tweet_name, since_id = since_id, count = 200, max_id = max_id)\n",
    "    tweets.extend(new_tweets)\n",
    "    max_id = tweets[-1].id - 1\n",
    "  \n",
    "  tweets = [[tweet.id_str, tweet.created_at, tweet.text, \"\", \"\", \"\",tweet.retweet_count, tweet.favorite_count] for tweet in tweets]\n",
    "  logger.info(\"Downloading %d tweets from %s\" % (len(tweets), tweet_name))\n",
    "  return tweets[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lists(df):\n",
    "  # put twitter handles, last acquired tweet ID, tweet count and store them in respective lists\n",
    "  names = filter(lambda x: x > 0, df.iloc[:, 1])\n",
    "  max_ids = df.iloc[:, 2]\n",
    "  counts = df.iloc[:, 3]\n",
    "  \n",
    "  # save the number of entries\n",
    "  indices = range(1,len(names)+1)\n",
    "  \n",
    "  lists = zip(names, max_ids, counts, indices)\n",
    "  del lists[0] # the first one is column title\n",
    "  return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sheets(path):\n",
    "  sheet_book = load_workbook(path)\n",
    "  sheet_writer = pd.ExcelWriter(path, engine='openpyxl')\n",
    "  sheet_writer.book = sheet_book\n",
    "  sheet_writer.sheets = dict((ws.title, ws) for ws in sheet_book.worksheets)\n",
    "  logger.info(\"Downloaded %s\" % path)\n",
    "  return sheet_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be finished\n",
    "def illchar(par):\n",
    "  if (name == 'jimananich'):\n",
    "    writer = pd.ExcelWriter('id_.xlsx')\n",
    "    _id = pd.Series(df.iloc[:,0]).astype(str)\n",
    "    _id.to_excel(writer, sheet_name='id', header=False, index=False, float_format='string')\n",
    "    writer.save()\n",
    "    df.to_csv('illchar.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to Sheets â†“\n",
    "\n",
    "The functions below write data to the currently local sheets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Tweets and save them to respective excel file\n",
    "\n",
    "*twitter_list.xlsx* contains the list of candidates for each tweets excel file along with the metadata.\n",
    "\n",
    "*cand_tweets.xlsx* contains the tweets for all announced candidates\n",
    "\n",
    "*spec_tweets.xlsx* contains the tweets for all the speculated candidates\n",
    "\n",
    "*rep_tweets.xlsx* contains the tweets for all current CA represenatives of interest for the CA 2018 Elections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# collect_data(tweet_sheet, twitter_list, twitter_sheet)\n",
    "# This function pulls new latest tweets and appends them to the correct excel file\n",
    "# params: tweet_sheet - the path to the excel file to save the new tweets \n",
    "#         twitter_list - path to file that contains the twitter handles, last tweet pulled id, tweet counts, and \n",
    "#                        last pull date\n",
    "#         twitter_sheet - the correct sheet of accounts corresponding to the tweet sheet passed in\n",
    "# returns: n/a\n",
    "def collect_data(tweet_sheet, twitter_list, twitter_sheet):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweets_path = os.path.expanduser(tweet_sheet)\n",
    "  twitter_path = os.path.expanduser(twitter_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(twitter_path)\n",
    "  list_df = pd.read_excel(twitter_path, twitter_sheet)\n",
    "  \n",
    "  # list_df['Last_Pulled'] = pd.to_datetime(list_df['Last_Pulled'], errors='coerce') \n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweets_path)\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():       \n",
    "    name, since_id, count = row[1], row[2],row[3]\n",
    "    \n",
    "    # check if rep has a twitter handle \n",
    "    if (type(name) is not unicode):\n",
    "      continue\n",
    "    \n",
    "    new_tweets = get_new_tweets(name, since_id)\n",
    "    # if there are no new tweets continue to the next account\n",
    "    if (len(new_tweets) > 0):\n",
    "      # turn the new tweets into a dataframe and write them to the corresponding excel sheet\n",
    "      df = pd.DataFrame(new_tweets)\n",
    "      \n",
    "      df.to_excel(tweet_writer, sheet_name=name, startrow=count+1, header=False, index=False, float_format='string')\n",
    "      \n",
    "      # update since_id, count, and last_pull date in tweet list\n",
    "      list_df.iat[index,2] = new_tweets[len(new_tweets)-1][0] # since_id\n",
    "      list_df.iat[index,3] = count + len(new_tweets) # last_pull\n",
    "      list_df.iat[index,4] = pd.to_datetime(time.strftime(\"%m/%d/%Y %H:%M:%S\"), errors='coerce') # last_pull date\n",
    "      \n",
    "      logger.info(\"Updated new tweets on spreadsheet for %s\" % name)\n",
    "      time.sleep(20)\n",
    "  \n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  list_df.Last_ID = list_df.Last_ID.astype(str)\n",
    "  list_df.to_excel(list_writer, sheet_name=twitter_sheet, index=False) #, float_format='string')\n",
    "  list_writer.save()\n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done appending new tweets for %s\", twitter_sheet)\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Updates like and retweet totals\n",
    "def collect_addition_data(tweet_sheet, tweet_list, sheetname):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweet_sheet = os.path.expanduser(tweet_sheet)\n",
    "  tweet_list = os.path.expanduser(tweet_list)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheet_name=sheetname)\n",
    "  list_df = list_df.dropna(thresh=4)\n",
    "  # properly load spreadsheet to append new data\n",
    "  tweet_writer = load_sheets(tweet_sheet)\n",
    "  logger.info(\"Downloaded tweets list\")\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for row in list_df.itertuples():  \n",
    "    name, since_id, count = row[2], row[3],row[4]\n",
    "    \n",
    "    # read cand tweet sheet\n",
    "    tweets_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "    logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "    \n",
    "    # retreive updated tweets\n",
    "    tweets = get_new_tweets(name, 1)\n",
    "    updates_df = pd.DataFrame(tweets)\n",
    "    \n",
    "    if (updates_df.empty):\n",
    "      continue \n",
    "      \n",
    "    # clean dataframe to only include id, retweets, and favorites\n",
    "    updates_df = updates_df[[0, 6, 7]]\n",
    "    updates_df.columns = ['id', 'retweets', 'favorites']\n",
    "    \n",
    "    # call helper fuction to match updated metadata with correct tweets\n",
    "    tweets_df = update_metadata(tweets_df, updates_df, name)\n",
    "    \n",
    "    if (name == 'antonio4ca'):\n",
    "      continue\n",
    "      \n",
    "    # write the updated data to the twitter profile's sheet to be saved\n",
    "    tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=1)\n",
    "    logger.info(\"Updated data on spreadsheet for %s\" % name)\n",
    "    # 100 second pause between data pulls to avoid token exceptions\n",
    "    time.sleep(20)\n",
    "  \n",
    "  tweet_writer.save()\n",
    "  \n",
    "  logger.info(\"Done collecting additional data\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the up to date metadata and matches it to their respective tweet using a tweet's unique id\n",
    "# currently not in use\n",
    "def update_metadata(tweets_df, updates_df, cand_name): \n",
    "  # convert tweet id to the same type as the updates sheet\n",
    "  tweets_df['id'] = tweets_df['id'].astype(str)\n",
    "  tweets_df.set_index('id', inplace=True)\n",
    "  \n",
    "  ## loop through the updates metadata and updates the tweet sheet\n",
    "  for row in updates_df.itertuples():\n",
    "    tweets_df.set_value(row[1], 'retweets', row[2])\n",
    "    tweets_df.set_value(row[1], 'favorites', row[3])\n",
    "\n",
    "  # drop null rows that could not match with a tweet\n",
    "  tweets_df.dropna(subset=['created_at'], inplace=True)\n",
    "  \n",
    "  return tweets_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_sheets(new_tweet_sheet, new_tweet_list, list_sheet_names):\n",
    "  # start the timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  new_tweet_sheet = os.path.expanduser(new_tweet_sheet)\n",
    "  new_tweet_list = os.path.expanduser(new_tweet_list)\n",
    "  \n",
    "  # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "  tweet_writer = pd.ExcelWriter(new_tweet_sheet)\n",
    "  \n",
    "  # load and prepare list of twitter accounts\n",
    "  list_writer = load_sheets(new_tweet_list)\n",
    "  \n",
    "  sheet_list = []\n",
    "  \n",
    "  for sheetname in list_sheet_names:\n",
    "    list_df = pd.read_excel(new_tweet_list, sheetname=sheetname)\n",
    "    list_df = list_df.dropna(axis=0, subset=['Twitter'])\n",
    "  \n",
    "    # \n",
    "    for row in list_df.itertuples():\n",
    "      name, since_id, count = row[2], row[3],row[4]\n",
    "      sheet_list.append(name)\n",
    "      \n",
    "      # Create a Pandas dataframe from some data.\n",
    "      col_headers = {'id': [np.nan], 'created_at': [np.nan], 'text': [np.nan], 'hashtag#': [np.nan], 'at@': [np.nan],\n",
    "                    'link': [np.nan], 'retweets': [np.nan], 'favorites': [np.nan], 'fullURL': [np.nan]}\n",
    "      tweets_df = pd.DataFrame(col_headers)\n",
    "      tweets_df = tweets_df[['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'fullURL']]\n",
    "      \n",
    "      # write the updated data to the twitter profile's sheet to be saved\n",
    "      tweets_df.to_excel(tweet_writer, sheet_name=name, index=False, startcol=0)\n",
    "    \n",
    "  tweet_writer.save()\n",
    "  logger.info(\"Done creating excel doc\")\n",
    "  # stop timer and print time elapsed for the current data pull\n",
    "  end = time.time()\n",
    "  logger.info(\"Time Elapsed: %d\", float((end-start))/60)\n",
    "  return sheet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_xlsx_csv (tweet_sheet, sheetname, tweet_list):\n",
    "  # start timer\n",
    "  start = time.time()\n",
    "  logger.info(\"Start...\")\n",
    "  # dp_client = authenticate_dropbox()\n",
    "  \n",
    "  # process the paths so they are passable to load_sheets\n",
    "  tweet_sheet = os.path.expanduser(tweet_sheet)\n",
    "  tweet_list = os.path.expanduser(tweet_list)  \n",
    "    \n",
    "  # load and prepare list of twitter accounts    \n",
    "  list_writer = load_sheets(tweet_list)\n",
    "  list_df = pd.read_excel(tweet_list, sheetname=sheetname)\n",
    "  #merged_corpus = pd.DataFrame(columns=['id', 'created_at', 'text', 'hashtag#', 'at@', 'link', 'retweets', 'favorites', 'full URL'])\n",
    "  merged_df = pd.DataFrame()\n",
    "\n",
    "  initial_loop = True\n",
    "  \n",
    "  # loop through the list of Cand/PACs and updates each tweet sheet appropriately\n",
    "  for index, row in list_df.iterrows():\n",
    "    name = row[0]\n",
    "    \n",
    "    if (initial_loop):\n",
    "      merged_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      merged_df['Name'] = name\n",
    "      num_tweets = len(merged_df.index)\n",
    "      print num_tweets\n",
    "\n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      initial_loop = False\n",
    "    \n",
    "    else:\n",
    "      # read current cand tweet sheet\n",
    "      curr_df = pd.read_excel(tweet_sheet, sheetname=name)\n",
    "      curr_df['Name'] = name\n",
    "      num_tweets = len(curr_df.index)\n",
    "      if (num_tweets == 0):\n",
    "        continue\n",
    "      \n",
    "      logger.info(\"Retrived data from spreadsheet for %s\" % name)\n",
    "      \n",
    "      merged_df = merged_df.append(curr_df)\n",
    "  \n",
    "  print merged_df.shape\n",
    "  # write the updated list and save the changes to the excel sheets\n",
    "  #merged_df.to_csv('merged_corpus.csv', encoding='utf-8')\n",
    "  \n",
    "  logger.info(\"done\")\n",
    "  return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_number_string(value):\n",
    "    if isinstance(value, (list, tuple)):\n",
    "        print str(value) + 'here'\n",
    "        return str(value)[1:-1]\n",
    "    else:\n",
    "        return value\n",
    "\n",
    "# sample_df['text'] = sample_df['text'].apply(list_to_number_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# func that will pull a number of tweets and create a coding sheet with columns for rating\n",
    "# sentitment, partisanship, policy, fact, and opinion\n",
    "def coding_sampling(coding_workbook, sheetname, sample_size, exclude):\n",
    "  logger.info(\"Start...\")\n",
    "  start = time.time()\n",
    "  \n",
    "  # properly load spreadsheet to append new data\n",
    "  coding_workbook = os.path.expanduser(coding_workbook)\n",
    "  sample_writer = load_sheets(coding_workbook)\n",
    "  full_df = pd.read_excel(coding_workbook, sheetname)\n",
    "  logger.info(\"Full data loaded\")\n",
    "  \n",
    "  col_headers = {'id': [str],'text': [np.nan], 'sentiment': [np.nan], 'political': [np.nan], 'ideology': [np.nan],\n",
    "                 'macroeconomics': [np.nan], 'national_security': [np.nan], 'ideology': [np.nan],\n",
    "                 'crime_law_enforcement': [np.nan], 'civil_rights': [np.nan], 'environment': [np.nan],\n",
    "                 'education': [np.nan], 'healthcare': [np.nan], 'governance': [np.nan], 'no_policy_content': [np.nan],\n",
    "                 'asks_for_donation': [np.nan], 'ask_to_watch_read_share_follow_something': [np.nan],\n",
    "                 'factual_claim': [np.nan], 'opinion': [np.nan]}\n",
    "  sample_df = pd.DataFrame(col_headers)\n",
    "  \n",
    "  # keep only id and text \n",
    "  full_df = full_df[['id', 'text']]\n",
    "  \n",
    "  # if (exclude):\n",
    "    # remove already coded tweets\n",
    "  \n",
    "  sample_tweets = full_df.sample(sample_size)\n",
    "  logger.info(\"Sample made\")\n",
    "  \n",
    "  sample_df = pd.concat([sample_tweets, sample_df])\n",
    "  sample_df = sample_df[['id', 'text', 'sentiment', 'political', 'ideology', 'macroeconomics', 'immigration',\n",
    "                         'national_security', 'crime_law_enforcement', 'civil_rights', 'environment', 'education',\n",
    "                         'healthcare', 'governance', 'no_policy_content', 'asks_for_donation',\n",
    "                         'ask_to_watch_read_share_follow_something', 'factual_claim', 'opinion']]\n",
    "  sample_df.drop(sample_df.tail(1).index,inplace=True) # drop last n rows\n",
    "  logger.info(\"Sample processed and concated\")\n",
    "  \n",
    "  sample_df.id = sample_df.id.astype(str)\n",
    "  sample_df.to_excel(sample_writer, 'batch2', index=False, float_format='string')\n",
    "  sample_writer.save()\n",
    "  \n",
    "  logger.info(\"done\")\n",
    "  #return sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_sampling(state_data_dir + coded_tweets, all_tweets, coding_sample_size, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reference code on how to save the data to an excel sheet and not lose id data\n",
    "writer = pd.ExcelWriter('coding_file.xlsx')\n",
    "merged_sheets.id = merged_sheets.id.astype(str)\n",
    "merged_sheets.to_excel(writer, 'total_sample', index=False, float_format='string')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Pull Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set file pathway variables an expand to HOME\n",
    "ca_data_dir = '~/Dropbox/Summer_of_Tweets/ca_working_sheets/'\n",
    "state_data_dir = '~/Dropbox/Summer_of_Tweets/State_Leg_Working_Data/'\n",
    "\n",
    "# the excel sheets containing the tweets\n",
    "cand_tweets = \"cand_tweets.xlsx\"\n",
    "spec_tweets = \"spec_tweets.xlsx\"\n",
    "rep_tweets = \"rep_tweets.xlsx\"\n",
    "state_tweets = \"state_tweets.xlsx\"\n",
    "coded_tweets = \"coding_file.xlsx\"\n",
    "\n",
    "# the excel file containing the accounts and its sheets\n",
    "twitter_list = \"Twitter_List.xlsx\" # this is the name of the metadata lists\n",
    "cand_sheet = \"cand\"\n",
    "spec_sheet = \"speculated\"\n",
    "rep_sheet = \"reps\"\n",
    "ca49_sheet = \"CA49\"\n",
    "state_sheets = [\"CA\", \"AK\", \"ME\", \"IL\", \"GA\", \"WY\", \"MI\", \"IN\",\"HI\", \"AL\", \"CT\", \"WA\", \"AZ\", \"FL\", \"NJ\", \"OR\", \"OK\",   \n",
    "                \"SD\", \"WI\", \"PA\"]\n",
    "all_handles = 'all'\n",
    "all_tweets = 'total_sample'\n",
    "\n",
    "coding_sample_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/ca_working_sheets/Twitter_List.xlsx\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/ca_working_sheets/cand_tweets.xlsx\n",
      "INFO:__main__:Downloading 71 tweets from ApplegateCA49\n",
      "INFO:__main__:Updated new tweets on spreadsheet for ApplegateCA49\n",
      "INFO:__main__:Downloading 321 tweets from SaraJacobsCA\n",
      "INFO:__main__:Updated new tweets on spreadsheet for SaraJacobsCA\n",
      "INFO:__main__:Downloading 1405 tweets from MikeLevinCA\n",
      "INFO:__main__:Updated new tweets on spreadsheet for MikeLevinCA\n",
      "INFO:__main__:Downloading 3 tweets from CPforCongress\n",
      "INFO:__main__:Updated new tweets on spreadsheet for CPforCongress\n",
      "INFO:__main__:Downloading 124 tweets from KristinDGaspar\n",
      "INFO:__main__:Updated new tweets on spreadsheet for KristinDGaspar\n",
      "INFO:__main__:Downloading 58 tweets from DianeHarkey\n",
      "INFO:__main__:Updated new tweets on spreadsheet for DianeHarkey\n",
      "INFO:__main__:Downloading 65 tweets from brianlmaryott\n",
      "INFO:__main__:Updated new tweets on spreadsheet for brianlmaryott\n",
      "INFO:__main__:Done appending new tweets for CA49\n",
      "INFO:__main__:Time Elapsed: 3\n"
     ]
    }
   ],
   "source": [
    "collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, cand_sheet)\n",
    "print \n",
    "collect_data(ca_data_dir + spec_tweets, ca_data_dir + twitter_list, spec_sheet)\n",
    "print \n",
    "collect_data(ca_data_dir + rep_tweets, ca_data_dir + twitter_list, rep_sheet)\n",
    "print\n",
    "collect_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start...\n",
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/ca_working_sheets/Twitter_List.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Downloaded /Users/SoloMune/Dropbox/Summer_of_Tweets/ca_working_sheets/spec_tweets.xlsx\n",
      "INFO:__main__:Downloaded tweets list\n",
      "/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/io/excel.py:329: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\n",
      "  **kwds)\n",
      "INFO:__main__:Retrived data from spreadsheet for Kevin_Faulconer\n",
      "INFO:__main__:Downloading 3232 tweets from Kevin_Faulconer\n",
      "/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/ipykernel_launcher.py:10: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-58a5ad855a57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#collect_addition_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, cand_sheet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcollect_addition_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mca_data_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mspec_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_data_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtwitter_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_sheet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcollect_addition_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mca_data_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrep_tweets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mca_data_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtwitter_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep_sheet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# <-- issues here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-11c6b3fc3ece>\u001b[0m in \u001b[0;36mcollect_addition_data\u001b[0;34m(tweet_sheet, tweet_list, sheetname)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# call helper fuction to match updated metadata with correct tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'antonio4ca'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-6727de54c7d2>\u001b[0m in \u001b[0;36mupdate_metadata\u001b[0;34m(tweets_df, updates_df, cand_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m## loop through the updates metadata and updates the tweet sheet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupdates_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'retweets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mtweets_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'favorites'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mset_value\u001b[0;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[1;32m   2570\u001b[0m                       \u001b[0;34m\".at[] or .iat[] accessors instead\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m                       stacklevel=2)\n\u001b[0;32m-> 2572\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtakeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_set_value\u001b[0;34m(self, index, col, value, takeable)\u001b[0m\n\u001b[1;32m   2585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2586\u001b[0m             \u001b[0;31m# set using a non-recursive method & reset the cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2587\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2588\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    374\u001b[0m                     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/util/_decorators.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mPY2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3564\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axis'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3565\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3566\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3568\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reindex_axis'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3687\u001b[0m         \u001b[0;31m# perform the reindex on the axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3688\u001b[0m         return self._reindex_axes(axes, level, limit, tolerance, method,\n\u001b[0;32m-> 3689\u001b[0;31m                                   fill_value, copy).__finalize__(self)\n\u001b[0m\u001b[1;32m   3690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3691\u001b[0m     def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_reindex_axes\u001b[0;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[1;32m   3499\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3500\u001b[0m             frame = frame._reindex_index(index, method, copy, level,\n\u001b[0;32m-> 3501\u001b[0;31m                                          fill_value, limit, tolerance)\n\u001b[0m\u001b[1;32m   3502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3503\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_reindex_index\u001b[0;34m(self, new_index, method, copy, level, fill_value, limit, tolerance)\u001b[0m\n\u001b[1;32m   3510\u001b[0m         return self._reindex_with_indexers({0: [new_index, indexer]},\n\u001b[1;32m   3511\u001b[0m                                            \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3512\u001b[0;31m                                            allow_dups=False)\n\u001b[0m\u001b[1;32m   3513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3514\u001b[0m     def _reindex_columns(self, new_columns, method, copy, level,\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   3808\u001b[0m                                                 \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3809\u001b[0m                                                 \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3810\u001b[0;31m                                                 copy=copy)\n\u001b[0m\u001b[1;32m   3811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3812\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   4412\u001b[0m         \u001b[0;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4414\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_reindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/SoloMune/miniconda2/envs/twitter-projects/lib/python2.7/site-packages/pandas/core/indexes/base.pyc\u001b[0m in \u001b[0;36m_can_reindex\u001b[0;34m(self, indexer)\u001b[0m\n\u001b[1;32m   3574\u001b[0m         \u001b[0;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3576\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3578\u001b[0m     def reindex(self, target, method=None, level=None, limit=None,\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    }
   ],
   "source": [
    "#collect_addition_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, cand_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + spec_tweets, ca_data_dir + twitter_list, spec_sheet)\n",
    "print\n",
    "collect_addition_data(ca_data_dir + rep_tweets, ca_data_dir + twitter_list, rep_sheet) # <-- issues here\n",
    "print\n",
    "collect_addition_data(ca_data_dir + cand_tweets, ca_data_dir + twitter_list, ca49_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for state in state_sheets:\n",
    "  collect_data(state_data_dir + state_tweets, state_data_dir + twitter_list, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
